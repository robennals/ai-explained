# Embeddings and Vector Spaces

<Lead>
In the previous chapter, we represented animals as vectors — lists of numbers capturing properties like size, speed, and danger. But AI systems don't just work with animals. They work with *everything*: words, images, sounds. How do you represent "anything" as a vector, when different kinds of things have completely different properties?
</Lead>

## The Problem with Hand-Picked Dimensions

When we represented animals as vectors, we chose dimensions that made sense for animals: size, speed, danger. But what dimensions would you use for *all words*? "How big is the color blue?" doesn't make sense. "How fast is democracy?" There's no universal set of labeled dimensions that works for everything.

Even within things you *can* measure, two dimensions quickly run into trouble. Let's see what happens when we try to organize different kinds of things in a 2D space.

<Simple2DScatterWidget />

<TryIt>
In "Animal or Food?" — chicken, salmon, and lamb appear in the overlap because they belong to both categories. Two dimensions can represent this, like a Venn diagram. Try "Four Distinct Categories" — animals, buildings, planets, and instruments sit in clean separate quadrants. Then try "Four Overlapping Categories" — wild, water, land, and domestic animals overlap in every direction, and two dimensions can't untangle them.
</TryIt>

With just two numbers, we can represent overlapping categories like a Venn diagram, or separate four distinct groups into quadrants. But we can't distinguish everything — when categories overlap in complex ways, two dimensions aren't enough. We'd need more: "is it a pet?", "does it have strings?", "is it edible?"

## Beyond Two Dimensions

Each new dimension adds discriminative power. With 5 dimensions, you could separate animals from vehicles from instruments from food. With 20 dimensions, you could tell a dog from a cat. With 50 or 100 dimensions, you could capture remarkably fine-grained distinctions.

Real AI systems solve the dimension problem by not using labeled dimensions at all. Instead, they learn a space where **similar words end up nearby** and **different words end up far apart**. The dimensions are just numbers — not "alive?" or "big?" — but the proximity structure captures meaning.

And it's not just proximity. The spatial *relationships* between nearby words often carry meaning too. In a well-trained embedding, the direction from "small" to "big" tends to be consistent — move an ant in that direction and you get larger animals. The direction from "man" to "woman" captures gender, so applying it to "king" lands you near "queen." These regularities aren't designed in; they emerge from training, because a space with consistent structure is more useful than one without it.

This is like the difference between organizing books by (genre, length, year) versus letting a librarian arrange them so that related books are on nearby shelves. The librarian's system has no labels, but it works better because it captures relationships that no fixed labeling scheme could.

<KeyInsight>
Real embeddings don't use labeled dimensions. They learn positions where similar things are nearby and where directions between words encode consistent relationships — size, gender, tense, and countless others — all without any human labeling.
</KeyInsight>

## Neural Networks Meet Embeddings

In the previous chapters, you built neurons that classified inputs by drawing a decision boundary. Those inputs were abstract numbers. But now each input has a name — its position in the embedding encodes meaning. Can a neuron learn to read that meaning?

Let's find out. Below is a single neuron whose two inputs are a word's coordinates in a simple 2D embedding (size and danger). Each word sits at its embedding position, and the neuron has to learn which words match a concept.

<EmbeddingClassifierWidget />

<TryIt>
Start with "Dangerous" — the neuron needs to draw a boundary separating the dangerous things (bear, shark, knife, gun) from everything else. Hit Optimize and watch it learn. Then try "Big" — it draws a vertical boundary instead. Try "Big & Dangerous" and "Small & Dangerous" too — notice how the neuron tilts its boundary line differently each time to separate the right words.
</TryIt>

This is only a toy example with two dimensions — but the same principle scales up. Real language models feed high-dimensional embeddings (hundreds or thousands of numbers per word) into neural networks with many layers, and those networks learn to read meaning from the geometry of the embedding space.

## Exploring a Real Embedding

Let's explore an actual word embedding — GloVe, trained by Stanford on 6 billion words of text. Each word is a point in 50-dimensional space. We can't visualize 50 dimensions, but we *can* ask: what's nearby?

<EmbeddingPlaygroundWidget />

<TryIt>
Start with "dog" — its neighbors are other animals and pets. Try "guitar" — you get other instruments. Try "queen" — notice it sits near royalty words. Type your own words and see what clusters together.
</TryIt>

Notice that the embedding groups words by meaning without being told what any word means. "Dog" is near "cat" and "puppy". "Guitar" is near "piano" and "bass". This structure emerged just from reading text — the system learned that words appearing in similar contexts should be nearby.

## Directions Have Meaning

The embedding doesn't just group similar words together — *directions* in the space carry meaning too. The direction from "ant" to "whale" captures something like size. The direction from "boy" to "man" captures age. You can pick any two words, and the line between them defines a meaningful spectrum.

<WordPairSpectrumWidget />

<TryIt>
Try "ant ↔ whale" — notice how animals arrange from small to large between the endpoints. Try "salad ↔ cake" for a healthy-to-indulgent food spectrum. Try "boy ↔ man" for an age spectrum. Pick your own word pairs and see what spectrums emerge.
</TryIt>

This is why unlabeled dimensions are so powerful. Even though no dimension is labeled "size" or "age," those concepts exist as directions in the space. And there are far more meaningful directions than there could ever be labeled dimensions — every pair of words defines a potential axis of meaning.

<KeyInsight>
In a well-trained embedding, directions carry meaning. The direction from small to large, from young to old, from simple to complex — these all exist as paths through the space. This structure emerges from training on text, with no human labeling.
</KeyInsight>

## Where Do Embeddings Come From?

Two key ideas drive how embeddings are created:

**Learned from context.** Systems like Word2Vec and GloVe are trained on the principle that "a word is known by the company it keeps." Words that appear near the same other words get similar vectors. "Dog" and "cat" both appear near "pet", "fur", "veterinarian" — so they end up close together in the embedding.

**Learned for a task.** Inside a neural network, an embedding layer is simply a lookup table — a matrix with one row per word and one column per dimension. To get the embedding for a word, you look up its row. The matrix starts random and is shaped by backpropagation, just like any other set of weights. The network learns whatever representation helps it perform its task.

Modern language models use much higher dimensions than our 50-dimensional GloVe example: GPT-2 uses 768 dimensions, GPT-3 uses 12,288 dimensions. And they embed not individual words but *tokens* — subword pieces that let the model handle any text, including words it has never seen before.

<EmbeddingLayerDiagramWidget />

<TryIt>
Click different words and watch the highlighted row change. Toggle between "Before training" (random numbers) and "After training" (structured vectors from a real embedding). Before training, the vectors are meaningless noise. After training, similar words have similar patterns. The network learned this structure by adjusting the numbers to be useful for its task.
</TryIt>

## Beyond Words

Embeddings are one of the most powerful ideas in modern AI. The pattern — take something discrete, map it to a point in continuous space, let training shape the geometry — applies far beyond words:

- **Tokens** (subword pieces) in language models like GPT
- **Image patches** in vision models
- **Audio frames** in speech recognition
- **Users and products** in recommendation systems
- **Molecules** in drug discovery

In every case, the same principle holds: similar things end up nearby, and the geometry of the space encodes meaningful relationships. An embedding is a learned translation from symbols to geometry — and the geometry of meaning emerges from the pressure to be useful.

<KeyInsight>
An embedding is a learned mapping from discrete symbols to continuous geometry. The network discovers what dimensions to use and what structure to create — driven entirely by the need to perform its task well. This idea extends far beyond words to images, audio, molecules, and more.
</KeyInsight>

# Neural Networks

<Lead>
Your brain is a neural network — 86 billion neurons connected by roughly 100 trillion synapses. GPT, Claude, and every other modern AI system are also neural networks, built from artificial versions of the same idea. They can write essays, generate images, translate languages, and hold conversations. You might expect the technology behind all of this to be fearsomely complex. It isn't. It's remarkably simple.
</Lead>

<NeuronScaleComparisonWidget />

But notice something strange in this chart. GPT-5 has about as many connections as a mouse — yet it can write essays and translate languages, and mice generally can't. And elephants and sperm whales have far more connections than humans, yet aren't smarter than us. The number of connections isn't everything — what matters most is **how good the weights are**. That's what training is all about, and we'll get deeper into it in later chapters.

## The Building Block

A neural network is made of **neurons**, connected together in layers.

<NetworkOverviewWidget />

Each neuron receives inputs from the previous layer, does a little computation, and passes a result to the next layer. But what's happening inside each one? Let's zoom in.

A neuron does two things:

1. **Weighted sum.** Each input gets multiplied by a **weight** — a number that controls how much that input matters. All the weighted inputs get added together, plus a constant called the **bias**. This is just multiplication and addition: nothing fancy yet.
2. **Activation function.** The result gets passed through a function that determines how much to pass on to the next layer. This is called the **activation function**, and it's what makes neurons more than just arithmetic. We'll see exactly why it matters later in this chapter.

That's the whole thing. A neuron is a weighted sum followed by an activation function.

<NeuronDiagramWidget />

We said neural networks are remarkably simple, and they are. The structure of a neuron is just multiplication, addition, and passing the result through a function. Wire millions of them together and you have GPT — or the human brain. The hard part isn't building the network — it's finding the right values for all those weights and biases. That's what training is for, and we'll get deeper into it in later chapters. For now, let's understand what a single neuron can do.

One common activation function is the **sigmoid** — an S-shaped curve that smoothly squashes any number into the range 0 to 1. Large positive inputs become close to 1. Large negative inputs become close to 0. Everything in between gets a smooth, graded value. It's a "soft switch": instead of snapping between off and on, it dims smoothly between them. We'll use sigmoid throughout this chapter, and meet other activation functions later.

The key property: when you change a weight by a small amount, the output changes by a small amount. No sudden jumps, no unpredictable flips. That's exactly what Chapter 2 told us we need for optimization to work.

## Neurons as Logic

You may be familiar with basic logical operations like **AND**, **OR**, and **NOT**. These combine true-or-false values:

- **AND**: "Are *both* things true?" You need an umbrella if it's raining AND you're going outside.
- **OR**: "Is *at least one* thing true?" You'll get wet if it's raining OR someone sprays you with a hose.
- **NOT**: Flips the answer. NOT true is false, NOT false is true.

These simple operations are the basis for every digital computer. Inside every computer chip is a network of **logic gates** — tiny components that compute AND, OR, NOT, and similar operations on electrical signals. Wire enough of them together and you can compute anything: that's how your phone, your laptop, and every digital device works.

But logic gates have a problem. Their outputs snap between 0 and 1 with nothing in between. There's no smooth gradient to follow, which means you can't train a network of logic gates using the optimization techniques from Chapter 2.

A neuron is like a **smooth version of a logic gate**. With the right weights, a single two-input neuron can compute AND, OR, NOT, and more — but unlike a logic gate, its output changes smoothly when you adjust the weights. Let's see this in action:

<NeuronPlaygroundWidget />

<TryIt>
Start by sliding the input values and watching how the output changes. Then try the gate challenges: can you find weight and bias settings that make the neuron compute AND? Press "Show" on any challenge to see one solution. Then try XOR — you won't find settings that work. We'll see why soon.
</TryIt>

But there's something important here beyond just mimicking logic gates. Notice what happens when you slide an input to a value *between* 0 and 1 — say 0.6. The output isn't forced to snap to "true" or "false." It can be somewhere in between: 0.73, or 0.4, or any other value. The neuron handles *degrees* naturally.

This matters because real-world data is rarely cleanly true or false. Is this email spam? Probably. Is that shadow in the photo a cat? Maybe. Is this patient sick? The test says 0.7 likely. A neuron can take uncertain inputs and produce a graded output — "probably true" rather than just "true" or "false."

<KeyInsight>
**A neuron is a weighted sum plus an activation function.** With the right weights, even a simple two-input neuron can compute AND, OR, NOT, and more. But neurons aren't limited to binary logic — they naturally handle in-between values, making them building blocks for computation over uncertain, continuous data. And because small changes to the weights produce small changes in the output, they're exactly the kind of model that optimization can work with.
</KeyInsight>

## Drawing Lines

Why can't a single neuron do XOR? To understand, let's look at what a neuron is *geometrically*.

Plot the four input combinations on a 2D plane: (0,0), (0,1), (1,0), (1,1). The neuron's weighted sum — `wA x A + wB x B + bias` — defines a straight line through this plane. On one side of the line, the output is near 1. On the other side, near 0. The line itself is where the neuron is exactly 50/50 — the **decision boundary**.

AND is easy: draw a line that puts (1,1) on one side and everything else on the other. OR is easy: put (0,0) alone on one side. But XOR asks you to put (0,1) and (1,0) together on one side, with (0,0) and (1,1) on the other — and these points are on *opposite corners*. No single straight line can do it.

This is called **linear separability**: a problem is linearly separable if you can solve it with a single straight line (or, in higher dimensions, a flat surface). AND and OR are linearly separable. XOR is not.

<DecisionBoundaryExplorerWidget />

<TryIt>
Drag the line to solve AND — isolate the (1,1) corner from everything else. Then solve OR. Then try XOR: put (0,1) and (1,0) together while keeping (0,0) and (1,1) apart. No matter how you rotate or shift the line, you can't get all four right. Scroll to rotate, drag to move. XOR is fundamentally not a straight-line problem.
</TryIt>

<Callout type="info" title="The XOR Crisis">
In 1969, Minsky and Papert published *Perceptrons*, proving that single-layer networks can't solve XOR. This was taken as proof that neural networks were a dead end. It killed most neural network research for over a decade — the "AI winter." The fix was always sitting there, obvious in hindsight: use more than one layer.
</Callout>

## Breaking Through

If one neuron draws one line, what happens when you use *two* neurons and combine their outputs?

Two lines. And with two lines, you can carve out regions that no single line could isolate.

Here's one way to build XOR from neurons:
- Neuron A computes OR: "is *at least one* input on?"
- Neuron B computes NAND (not-and): "are they *not both* on?"
- Output neuron computes AND of A and B: "at least one is on, but not both."

That's XOR. Three neurons, two layers — problem solved.

This is the fundamental insight about depth: **each layer transforms the data into a new representation where the next layer's job is easier.** The hidden layer doesn't solve the problem directly. It reshapes it into a problem that a single line *can* solve.

<XORBreakthroughWidget />

<TryIt>
Switch from 1 layer to 2 layers. Watch the right panel — it shows what the hidden layer "sees." The four input points have been *moved*: in this new space, the two XOR-true cases are on one side and the two XOR-false cases are on the other. The hidden layer transformed an impossible problem into an easy one.
</TryIt>

<KeyInsight>
**Depth transforms problems.** A single neuron draws one line. A hidden layer transforms the entire input space — stretching, folding, rearranging the points — so that what was previously inseparable becomes separable. Each layer makes the next layer's job easier. This is why neural networks are built in layers.
</KeyInsight>

## The Activation Trick

You might be thinking: great, so more layers = more power. Let's just stack a hundred layers and solve everything!

But there's a catch — and it's the most important thing in this chapter.

What happens if you remove the activation function? A neuron without an activation function is just: `output = wA x A + wB x B + bias`. That's a **linear** function — it just multiplies and adds. And here's the thing about linear functions: **a linear function of a linear function is still a linear function.**

Layer 1 takes inputs and multiplies and adds. Layer 2 takes those results and multiplies and adds. But the whole thing simplifies to... one multiplication and one addition. You could collapse a hundred layers into a single layer with different numbers. No matter how deep you go, you're still drawing a straight line.

It's like multiplying a number by 3, then by 5, then by 2. You could have just multiplied by 30. Adding more multiplications never gets you anything beyond one multiplication.

The activation function *breaks* this. Because it bends the curve — it's not a straight line — two layers with activation are genuinely more powerful than one. This nonlinearity is what makes depth meaningful. Remove it, and no matter how elaborate your architecture looks, you're still stuck with a single straight line.

<LinearCollapseDemoWidget />

<TryIt>
Turn off the activation function and set depth to 5 layers. Hit Train. No matter how long it runs, the boundary stays straight — five layers of nothing is still nothing. Now turn activation back on, keep 5 layers, hit Train. Watch the boundary bend and curve around the data. The activation function isn't decoration. It's the entire reason depth works.
</TryIt>

<KeyInsight>
**Without activation functions, depth is an illusion.** A stack of linear layers always collapses to a single linear layer — the decision boundary is a straight line no matter how deep you go. The nonlinear activation function is what makes each layer *genuinely add* to the network's expressive power.
</KeyInsight>

## What Can Neurons Compute?

Neurons with activation functions, stacked in layers, can curve the decision boundary. But *how much* can they curve it? What can they actually compute?

The answer, surprisingly, is: **anything**. This is the **universal approximation theorem** — a neural network with enough neurons can approximate any continuous function to any desired accuracy. In theory, even a single hidden layer suffices. In practice, "enough neurons" might mean millions. Adding more layers lets you do the same job with far fewer neurons — depth is exponentially more efficient than width.

Let's see this in action. Below, you can place points on a canvas — blue and orange — and watch a neural network learn to classify them in real time. Draw any pattern you like: a circle inside a ring, a zigzag, a smiley face. Then see how the architecture affects what the network can learn.

<NeuralNetworkTrainerWidget />

<TryIt>
Load the "circle" preset — blue points surrounded by orange. With 1 layer, the boundary can't wrap around the circle. Switch to 2 layers and watch it learn the ring shape. Now try "spirals" — that's the classic hard problem. You'll need at least 3 layers and more neurons. Try drawing your own pattern and experimenting with the architecture controls.
</TryIt>

<Callout type="info" title="Regression vs. Classification">
The point-classification mode above predicts *categories* — blue or orange. That's **classification**. But the same neural network architecture can also predict *numbers* — how high is a curve at this point? That's **regression**. Same architecture, same training algorithm. The only difference is how you interpret the output.
</Callout>

## Not All Activations Are Equal

We've been using sigmoid as our activation function. But there are other options, and the choice matters a lot for deep networks.

The problem shows up during training. As we saw in Chapter 2, training works by figuring out which direction to adjust each weight to reduce the error. In deep networks, this signal has to travel backward through every layer. And some activation functions make this signal fade away as it passes through.

Sigmoid has flat regions near 0 and 1 — once you're deep in the flat part, the signal for "which way to adjust" is nearly zero. Stack several layers of this, and the learning signal evaporates before it reaches the early layers. This is called the **vanishing gradient problem**.

**ReLU** — `max(0, x)` — fixes this for positive inputs. It's the simplest possible activation: just cut off anything below zero. For positive values, the signal passes through at full strength. But if a neuron's input goes permanently negative, it outputs 0 and can never recover. It's a **dead neuron**.

**Swish** — `x x sigmoid(x)` — is a modern compromise that avoids dead neurons while keeping signal flowing.

<ActivationFunctionExplorerWidget />

<TryIt>
Set depth to 8 layers with sigmoid. Hit Train and watch the gradient heatmap — the early layers (top rows) are dark, meaning the learning signal has faded to nearly zero. They're barely learning. Switch to ReLU: the signal flows better. Switch to Swish: signals flow *and* fewer neurons die. This is why modern networks rarely use sigmoid as their activation.
</TryIt>

## A Neuron's Family Tree

The artificial neuron was inspired by biology. In 1943, McCulloch and Pitts proposed a mathematical model of a brain cell: it takes inputs from other neurons, each with a different strength (weight), sums them up, and fires if the total exceeds a threshold. Sound familiar?

Real neurons are far more complex — they communicate with precisely timed electrical spikes, have intricate branching structures, and modify their connections through mechanisms that look nothing like the training algorithms we use. The artificial neuron is a cartoon of a brain cell. But it's a cartoon that turned out to be extraordinarily powerful.

What's more surprising is how many fields independently discovered the same math. A single neuron with a sigmoid activation is closely related to **logistic regression**, a statistical technique from the 1800s. Statisticians, neuroscientists, and AI researchers all arrived at essentially the same equation from completely different starting points.

## What's Next

We've met the building block — the neuron — and seen that it's a configurable logic operation with graded outputs. We've seen that stacking them in layers enables complexity, but *only* because the activation function prevents those layers from collapsing into one. We've watched neural networks learn to classify any pattern we can draw, and seen how the choice of activation function affects training.

But so far we've treated each layer as a box that "does something" to its inputs. What is that something, exactly? It turns out every layer is performing a geometric transformation — stretching, rotating, and folding the data in space. In the next chapter, we'll see the math behind that transformation, and discover a surprising connection: the same operations that power neural networks are exactly what video game graphics have been doing for decades.

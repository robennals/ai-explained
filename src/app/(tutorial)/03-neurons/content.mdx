# Neural Networks

<Lead>
Your brain is a neural network — 86 billion neurons connected by roughly 100 trillion synapses. GPT, Claude, and every other modern AI system are also neural networks, built from artificial versions of the same idea. They can write essays, generate images, translate languages, and hold conversations. You might expect the technology behind all of this to be fearsomely complex. It isn't. It's remarkably simple.
</Lead>

<NeuronScaleComparisonWidget />

But notice something strange in this chart. GPT-5 has about as many connections as a mouse — yet it can write essays and translate languages, and mice generally can't. And elephants and sperm whales have far more connections than humans, yet aren't smarter than us. The number of connections isn't everything — what matters most is **how good the weights are**. That's what training is all about, and we'll get deeper into it in later chapters.

## The Building Block

A neural network is made of **neurons**, connected together in layers.

<NetworkOverviewWidget />

Each neuron receives inputs from the previous layer, does a little computation, and passes a result to the next layer. But what's happening inside each one? Let's zoom in.

A neuron does two things:

1. **Weighted sum.** Each input gets multiplied by a **weight** — a number that controls how much that input matters. All the weighted inputs get added together, plus a constant called the **bias**. This is just multiplication and addition: nothing fancy yet.
2. **Activation function.** The result gets passed through a function that determines how much to pass on to the next layer. This is called the **activation function**, and it's what makes neurons more than just arithmetic. We'll see exactly why it matters later in this chapter.

That's the whole thing. A neuron is a weighted sum followed by an activation function.

<NeuronDiagramWidget />

We said neural networks are remarkably simple, and they are. The structure of a neuron is just multiplication, addition, and passing the result through a function. Wire millions of them together and you have GPT — or the human brain. The hard part isn't building the network — it's finding the right values for all those weights and biases. That's what training is for, and we'll get deeper into it in later chapters. For now, let's understand what a single neuron can do.

One common activation function is the **sigmoid** — an S-shaped curve that smoothly squashes any number into the range 0 to 1. Large positive inputs become close to 1. Large negative inputs become close to 0. Everything in between gets a smooth, graded value. It's a "soft switch": instead of snapping between off and on, it dims smoothly between them. We'll use sigmoid throughout this chapter, and meet other activation functions later.

The key property: when you change a weight by a small amount, the output changes by a small amount. No sudden jumps, no unpredictable flips. That's exactly what Chapter 2 told us we need for optimization to work.

## Neurons as Logic

You may be familiar with basic logical operations like **AND**, **OR**, and **NOT**. These combine true-or-false values:

- **AND**: "Are *both* things true?" You need an umbrella if it's raining AND you're going outside.
- **OR**: "Is *at least one* thing true?" You'll get wet if it's raining OR someone sprays you with a hose.
- **NOT**: Flips the answer. NOT true is false, NOT false is true.

These simple operations are the basis for every digital computer. Inside every computer chip is a network of **logic gates** — tiny components that compute AND, OR, NOT, and similar operations on electrical signals. Wire enough of them together and you can compute anything: that's how your phone, your laptop, and every digital device works.

But logic gates have a problem. Their outputs snap between 0 and 1 with nothing in between. There's no smooth gradient to follow, which means you can't train a network of logic gates using the optimization techniques from Chapter 2.

A neuron is like a **smooth version of a logic gate**. With the right weights, a single two-input neuron can compute AND, OR, NOT, and more — but unlike a logic gate, its output changes smoothly when you adjust the weights. Let's see this in action:

<NeuronPlaygroundWidget />

<TryIt>
Start by sliding the input values and watching how the output changes. Then try the gate challenges: click AND to see the target outputs. Can you find weight and bias settings that match? If you get stuck, hit Optimize and watch gradient descent find the right weights automatically. Then try XOR — even optimization can't solve it with one neuron.
</TryIt>

But there's something important here beyond just mimicking logic gates. Notice what happens when you slide an input to a value *between* 0 and 1 — say 0.6. The output isn't forced to snap to "true" or "false." It can be somewhere in between: 0.73, or 0.4, or any other value. The neuron handles *degrees* naturally.

This matters because real-world data is rarely cleanly true or false. Is this email spam? Probably. Is that shadow in the photo a cat? Maybe. Is this patient sick? The test says 0.7 likely. A neuron can take uncertain inputs and produce a graded output — "probably true" rather than just "true" or "false."

When you clicked Optimize, the neuron found the right weights automatically. How? It uses the same idea from Chapter 2: define a **loss** that measures how wrong the outputs are, then follow the gradient downhill. Because the neuron is smooth — small weight changes produce small output changes — the loss landscape has slopes you can follow. That's gradient descent: calculate how each weight affects the error, nudge every weight a little in the right direction, repeat. The activation function is what keeps everything smooth and trainable.

<KeyInsight>
**A neuron is a smooth, trainable logic gate.** With the right weights, it can compute AND, OR, NOT, and more — and because small changes to the weights produce small changes in the output, gradient descent can find those weights automatically. Neurons also handle in-between values naturally, making them building blocks for computation over uncertain, continuous data.
</KeyInsight>

## The Optimization Catch

You might wonder: if each parameter has a gradient telling it which way to go, why not just optimize them one at a time? Slide weight A to its best value, then slide weight B, then bias, and repeat.

It turns out this doesn't work well. The problem is that parameters interact. The best value for weight A depends on what weight B is set to, and vice versa. When you optimize A, you change the landscape for B. When you optimize B, you change the landscape for A. You end up zigzagging back and forth, making tiny progress.

<CoordinateDescentTrapWidget />

<TryIt>
Click "One at a time" and watch what happens: the optimizer zigzags back and forth along the narrow valley, making slow progress. Then click "Together" — gradient descent moves both parameters at once, cutting straight to the minimum. Or click "Compare both" to see them side by side.
</TryIt>

This is why gradient descent updates all parameters at once. Each parameter gets its own gradient, and they all move together in a single step. The combined movement follows the true downhill direction — not the misleading direction you'd get from optimizing one parameter at a time.

<KeyInsight>
**Gradient descent optimizes all parameters simultaneously.** Optimizing one at a time can get stuck because parameters interact — the best value for one depends on the others. Moving them all together follows the true downhill direction.
</KeyInsight>

## Beyond One Neuron

A single neuron can compute AND, OR, NOT, NAND, NOR — but not XOR. XOR asks: "is exactly one input on?" That requires two checks: "is at least one on?" (OR) AND "are they not both on?" (NAND). One neuron can only do one check. You need two neurons to do two checks, and a third to combine them.

That's two layers: a **hidden layer** that does the checking, and an **output layer** that combines the results. This is the fundamental insight — each layer performs a computation, and the next layer builds on it. One layer = one check. Two layers = checks combined. More layers = more complex combinations.

<Callout type="info" title="The XOR Crisis">
In 1969, Minsky and Papert published *Perceptrons*, proving that single-layer networks can't solve XOR. This was taken as proof that neural networks were a dead end, and it killed most neural network research for over a decade — the "AI winter." The fix was always sitting there, obvious in hindsight: use more than one layer.
</Callout>

In fact, neural networks with enough neurons in enough layers can compute **anything** — this is the **universal approximation theorem**. But having the right architecture isn't enough. You also need to find the right weights. With a single neuron we could just follow the loss curve downhill. But a network with thousands of neurons has thousands of weights. How do you optimize all of them at once?

## Training Deep Networks

The answer is **backpropagation** — an algorithm that figures out how each weight in the network contributes to the overall error, then nudges every weight in the right direction simultaneously.

The idea is elegant: start at the output and work backward. The output neuron knows its error directly. For the layer before it, we ask: "how much did each neuron contribute to that error?" For the layer before *that*, we ask the same question again. The error signal flows backward through the network, layer by layer, telling each weight which way to adjust.

This is just the chain rule from calculus, applied systematically. Each layer passes the training signal backward to the previous layer. The result: every weight in the network gets its own personalized nudge, all from a single forward pass and backward pass.

<NetworkTrainerWidget />

<TryIt>
Try XOR first with 1 layer — it can't solve it, no matter how long you train. Switch to 2 layers and hit Train: watch the classification regions bend and separate the classes. Now try Spirals — you'll need 3+ layers and more neurons per layer. Notice how more layers let the network learn more complex boundaries.
</TryIt>

<KeyInsight>
**Backpropagation makes deep networks trainable.** The training signal flows backward from the output through every layer, telling each weight which way to adjust. Combined with the smoothness of neurons, this means you can optimize networks with millions of weights — which is how modern AI works.
</KeyInsight>

## A Neuron's Family Tree

The artificial neuron was inspired by biology. In 1943, McCulloch and Pitts proposed a mathematical model of a brain cell: it takes inputs from other neurons, each with a different strength (weight), sums them up, and fires if the total exceeds a threshold. Sound familiar?

Real neurons are far more complex — they communicate with precisely timed electrical spikes, have intricate branching structures, and modify their connections through mechanisms that look nothing like the training algorithms we use. The artificial neuron is a cartoon of a brain cell. But it's a cartoon that turned out to be extraordinarily powerful.

What's more surprising is how many fields independently discovered the same math. A single neuron with a sigmoid activation is closely related to **logistic regression**, a statistical technique from the 1800s. Statisticians, neuroscientists, and AI researchers all arrived at essentially the same equation from completely different starting points.

## What's Next

We've seen the full picture: neurons are smooth computation units, and because they're smooth, you can optimize them. Stack them in layers and you can compute anything. Backpropagation lets you train all the weights at once, even in deep networks. This is the core of how modern AI works — the architecture is simple, and training finds the right weights.

But we've been training on tiny problems with a handful of data points. Real AI systems learn from millions of examples, and getting that learning process right is an art. In the next chapter, we'll see how training actually works at scale — what can go wrong, and the techniques that make it work.

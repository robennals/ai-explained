# Attention

<Lead>
In the last chapter, we saw that predicting the next word requires understanding — but our models could only look at a fixed window of recent words. What if the model could *choose* which earlier words to focus on? That idea, called attention, is the breakthrough that makes modern AI work. And it's built entirely from things you already know.
</Lead>

## Some Words Need Other Words

Read this sentence: "The dog chased the cat because **it** was angry."

What does "it" refer to? The dog or the cat? You probably said the dog — dogs chase things because *they're* angry, not the thing being chased. But notice what you just did: to understand one word ("it"), you had to look back at a *specific* earlier word ("dog") and use common-sense reasoning to pick it over another candidate ("cat").

This is something our fixed-window model from the last chapter can't do. It sees the last few words — maybe "because it was" — and has no way to reach back to "dog" at the start of the sentence. It doesn't get to *choose* which earlier words matter.

But for a human reader, this kind of selective looking-back happens constantly. Let's see just how much of language depends on it.

<WhyAttentionMattersWidget />

<TryIt>
Click the underlined words in each sentence. Notice how different words need different kinds of context: pronouns need their referents, ambiguous words need disambiguating clues, verbs need their subjects even when separated by long clauses. Flip through all five examples — each demonstrates a different reason why "looking back selectively" is essential.
</TryIt>

Every one of these examples requires the same ability: given a word, *choose* which other words in the sentence are relevant to understanding it. Not the nearest words. Not a fixed window. The *right* words, wherever they happen to be.

This is what **attention** does. It gives the model the ability to look at every word that came before and decide which ones matter for the word it's currently processing. Let's build up to how it works, step by step.

## Building Attention from Scratch

So how do we actually build a mechanism that does this? We need each word to be able to "ask a question" and find the other words that answer it. Let's start with the simplest possible example — so simple that every number is visible.

Imagine a tiny vocabulary of just four tokens: **cat**, **dog**, **bla** (filler), and **it** (a pronoun). Each token gets two vectors:

- A **key** — a short vector that advertises *what this token is*
- A **query** — a short vector that describes *what this token is looking for*

Our keys and queries are just two numbers. The first number represents "noun-ness" and the second represents "filler-ness":

<ToyVocabTableWidget />

The interesting one is **it**: its key is [0, 0] — it doesn't advertise itself as anything — but its query is [3, 0] — "I'm looking for a noun." We'll see why this matters in a moment.

To measure how well a query matches a key, we use the **dot product** from Chapter 5 — multiply corresponding numbers and add them up. A high dot product means a good match. Then we run the scores through a function called **softmax** (we'll explore how it works in the next section) that converts them into attention weights — positive numbers that add up to 100%.

<ToyAttentionWidget />

<TryIt>
Click each token to see its attention computed step by step. Start with "it" — watch how its query [3, 0] matches the noun's key [3, 0] (dot product = 9) but not the filler's key [0, 3] (dot product = 0). Then switch sentences — the noun moves to a different position, but "it" still finds it. That's the power of content-based attention.
</TryIt>

Notice something crucial about **it**: its key is [0, 0] — it doesn't advertise itself as anything — but its query is [3, 0] — "I'm looking for a noun." *What you are* isn't the same as *what you're looking for.* This is why we need separate queries and keys — a pronoun needs to find nouns without being mistaken for one itself.

## Softmax: Who's Shouting Loudest?

The toy example uses **softmax** to turn raw dot-product scores into attention weights. What does this function actually do, and why do we need it?

The dot products give us raw scores — but they can be any numbers: large, small, negative, zero. We need to convert them into weights that are all positive and add up to 100%, so we can use them to blend information. Softmax does this using exponentials: e raised to the power of each score. Since e^x is always positive, every weight is guaranteed to be positive. Then we divide by the total so everything sums to 1.

But softmax does something more interesting than just normalizing. Think of it as a competition: each score is a voice in a crowd, and what matters is **who's shouting loudest relative to everyone else**. If one score is moderately high and the others are near zero, it wins by default — even without being very confident. But if another score comes along much higher, it drowns out the moderate one and takes nearly all the weight. The *absolute* values matter less than the *gaps* between them.

<SoftmaxExplorerWidget />

<TryIt>
Start with "All equal" — everyone gets 25%. Then try "One clear winner" — A at 9 dominates completely. Now try "Close race" — the scores are close together, so weight is spread around. Drag slider A up slowly from 3 and watch it gradually take over. Then try making all scores large but equal (like 8, 8, 8, 8) — still 25% each! It's the differences that matter, not the magnitudes.
</TryIt>

## Values: Building New Representations

So far, attention tells us *where to look*. But we also need to know *what information to extract*. When "it" attends to "dog," what does it actually receive?

Each word has a third vector called a **value** — this represents the information the word *contributes* when attended to. So each word plays three roles:

- **Query (Q)**: "What am I looking for?"
- **Key (K)**: "What do I offer?"
- **Value (V)**: "Here's my information, if you need it."

The query and key determine *how much* attention to pay. The value is *what information* gets passed along. The output for each word is a weighted average of all the values, where the weights come from the attention scores.

Let's extend our toy example. We'll give each token a two-dimensional value vector that encodes its identity — a one-hot encoding of "which noun am I?":

- **cat**: V = [1, 0] — "I'm a cat"
- **dog**: V = [0, 1] — "I'm a dog"
- **bla**: V = [0, 0] — "I'm nothing interesting"
- **it**: V = [0, 0] — "I don't know what I am yet"

Now watch what happens when we run attention. Every token gets a new output — a weighted blend of all the values. The crucial insight: attention doesn't just *read* tokens — it **generates a new representation for every token**. After one pass of attention, "it" is no longer a generic pronoun with V = [0, 0] — its new representation carries the identity of whatever noun it found.

<ToyValuesWidget />

<TryIt>
All four outputs are shown at once — this is what the attention layer produces. Click **it** to see how its output absorbed the noun's identity: in "cat bla bla it," the output is nearly [1.00, 0.00] — cat's value. Switch sentences and watch the output change to match whichever noun is present. Then click **cat** or **bla** to see what happens to tokens that mostly attend to themselves.
</TryIt>

<KeyInsight>
Attention generates a **new representation** for every token by blending values from across the sentence. The original token "it" carries no information about which noun it refers to — but after attention, its output vector *encodes that noun's identity*. This is how context flows through the model: each layer of attention enriches every token with information from the tokens around it.
</KeyInsight>

## Attention Is Order-Blind

Now here's something that might surprise you. Let's take a sentence and scramble the word order:

<PositionScrambleWidget />

<TryIt>
Click "Scramble!" and look at the attention heatmap. The scores between words don't change! "Cat" and "dog" have the same attention score regardless of their positions. Try scrambling multiple times — the matrix values stay identical. This is a real problem.
</TryIt>

Wait — the attention scores are *exactly the same* whether the sentence is "The cat chased the dog" or "dog the chased cat The"? That can't be right. Those sentences mean completely different things!

The problem is that attention only looks at embeddings, and embeddings don't encode position. The word "cat" has the same embedding whether it's the first word or the last word. Attention has no idea *where* words are in the sentence — it only knows *what* they are.

We need to give the model information about word order. But how?

## Why Not Just Tell It the Distance?

Here's the obvious idea: just add a number to each token's embedding saying what position it's at. Position 1, position 2, position 3. Simple, right?

The trouble is: what matters in language is almost always *relative* position, not *absolute* position. "The word three positions before me" is meaningful; "the word at position 7" is not — it depends on where the sentence started. And what happens at position 10,001, a position the model has never encountered in training?

OK, so what about encoding the *distance* between pairs of tokens directly? After all, that's what we actually care about. The problem is structural. Attention computes a query and key for each individual token, then takes the dot product of every pair. That's a beautiful, efficient matrix multiplication: Q × K^T. But distance is a property of *pairs* of tokens — there are N² distances for N tokens. To inject per-pair distance information, you'd have to modify each of the N² dot products individually, which breaks the elegant matrix multiply. You'd be adding an entirely separate N×N table of corrections on top of the attention scores.

It turns out there are several clever ways to solve this problem. Different frontier models use different approaches, and each makes a different engineering tradeoff.

<PositionApproachesWidget />

<TryIt>
Click through all four approaches. Notice that they differ in *where* they inject position information: some modify the embeddings before attention happens, while others adjust the attention scores after the dot product. Pay particular attention to RoPE — it's the most widely used approach today.
</TryIt>

The first two approaches — learned absolute and sinusoidal — add position information directly to the token embeddings before attention even runs. This is simple but crude: the model can't easily separate "what this word means" from "where this word is."

ALiBi takes the opposite approach: leave the embeddings alone, and just penalize attention scores based on distance. Far-apart words get lower scores. This is elegant and generalizes well to longer sequences, but it's a blunt instrument — the penalty is the same regardless of content.

The most popular approach today — used by Llama, Mistral, Gemma, Qwen, DeepSeek, and most other frontier models — is **RoPE** (Rotary Position Encoding). It's more subtle than the others, and it's worth understanding in detail.

## The Rotation Trick

Here's the clever insight — a callback to Chapter 5 on matrix transformations.

Instead of *adding* a position number to each embedding, we **rotate** each word's query and key vectors by an angle that depends on position. Word at position 1 gets rotated a little. Word at position 5 gets rotated more. Word at position 100 gets rotated a lot more.

Why does this work? Remember from Chapter 5 that the dot product of two vectors depends on the angle between them. When we rotate two vectors by different amounts, the dot product depends on the *difference* in their rotation angles — which is exactly the *relative distance* between the words!

Two words that are 3 positions apart always have the same angular difference, whether they're at positions 1 & 4, or positions 50 & 53. The dot product automatically captures relative distance.

<RotationPositionWidget />

<TryIt>
Move the position sliders around. Keep the gap the same (say, 3) but change the absolute positions — the dot product stays the same! Now change the gap — the dot product changes. Click "Same gap, different positions" to see this clearly. The rotation trick encodes relative position into the dot product for free.
</TryIt>

Think of it like a merry-go-round. You and a friend are both riding it. No matter where the merry-go-round is in its spin, you can always tell how far apart you are by looking across. Your *relative* position is what matters, and it's encoded in the angle between you.

<KeyInsight>
By rotating query and key vectors based on position, the dot product automatically captures *relative distance* between words. The model doesn't need to know "I'm at position 5" — it naturally learns that "3 positions apart" always looks the same in the attention scores. And because the rotation is applied to Q and K *before* the dot product, it slots perfectly into the existing matrix multiplication — **no extra computation needed.**
</KeyInsight>

This is why RoPE has become the dominant approach. It adds no parameters, works for any sequence length, and the relative position information shows up exactly where we need it — in the dot products that attention already computes. It's the rare engineering solution that is both more elegant *and* more effective than the alternatives.

## Multiple Heads: Looking for Different Things

So far we've built one attention mechanism — one set of Q, K, and V weights that asks one kind of question. But understanding a sentence requires many kinds of questions at once: What does this pronoun refer to? What's the nearest noun? What's the subject of this verb? What clause does this word belong to?

Some of these questions — like "what's the nearest noun?" — depend on knowing word positions. That's why we needed positional encoding first. Now that the model knows both *what* words are and *where* they are, different attention heads can specialize in different patterns.

Attention models use **multiple attention heads** — each with its own Q, K, and V weights, each learning to look for different kinds of relationships. They all run in parallel on the same input, and their outputs get combined.

This isn't a theoretical idea — we can see it happening inside a real model. Below are attention weights from **BERT**, a well-known language model with 12 layers of 12 heads each (that's 144 attention heads total). We've picked four heads that learned strikingly different patterns:

<BertAttentionWidget />

<TryIt>
Switch between the four heads and click words to see their attention. Start with "Self / pronoun" and click "it" in the first sentence — watch it split attention between "dog" and "cat." Then switch to "Next word" — every word rigidly attends to the one after it. Try "Previous word" — the mirror image. Finally, "Broad context" shows wider structural connections. These are real attention weights from a real model — each head genuinely learned a different job.
</TryIt>

Each head is a specialist. The "Next word" and "Previous word" heads build a chain of local context — they only work because the model has positional encoding telling it where each word sits. The "Self / pronoun" head resolves references and links related concepts. The "Broad context" head captures sentence structure. No one told these heads what to specialize in — they discovered their roles through training, because different kinds of attention are all useful for predicting language.

<KeyInsight>
Multiple attention heads let the model ask many different questions in parallel. Each head has its own Q, K, and V weights, so each learns to attend to a different kind of relationship — position, pronouns, grammar, structure, semantics. **The model doesn't need to choose one kind of attention; it gets all of them at once.**
</KeyInsight>

## What We've Built

Let's take stock. Attention is:

1. **Query, Key, Value** — Each word gets three representations, learned from embeddings
2. **Dot products** — Measure how relevant each word is to each other word
3. **Softmax** — Turn relevance scores into percentages
4. **Weighted sum** — Blend values according to attention percentages
5. **Rotary positions** — Rotate embeddings so dot products automatically capture word distance
6. **Multiple heads** — Run several attention patterns in parallel, each specializing in different relationships

Every piece is something we already understood from earlier chapters. Attention is just a clever *wiring* of familiar operations.

In the next chapter, we'll wire attention together with feed-forward neural networks to build the **transformer** — the architecture behind ChatGPT, Claude, and every modern language model. And we'll see what happens when you train it to do that simple task from Chapter 6: predict the next word.

<TryItInPyTorch notebook="07-attention">
Compute dot-product attention from scratch, build query/key/value projections, visualize attention heatmaps on real sentences, implement multi-head attention, see how scrambled word order doesn't change attention scores, and build rotary position embeddings that encode relative distance.
</TryItInPyTorch>

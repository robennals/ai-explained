{/* Backup of old Chapter 3 sections — moved here during restructure */}
{/* These sections will be reused in a later chapter about matrices/geometry */}

## Drawing Lines

Why can't a single neuron do XOR? To understand, let's look at what a neuron is *geometrically*.

Plot the four input combinations on a 2D plane: (0,0), (0,1), (1,0), (1,1). The neuron's weighted sum — `wA x A + wB x B + bias` — defines a straight line through this plane. On one side of the line, the output is near 1. On the other side, near 0. The line itself is where the neuron is exactly 50/50 — the **decision boundary**.

AND is easy: draw a line that puts (1,1) on one side and everything else on the other. OR is easy: put (0,0) alone on one side. But XOR asks you to put (0,1) and (1,0) together on one side, with (0,0) and (1,1) on the other — and these points are on *opposite corners*. No single straight line can do it.

This is called **linear separability**: a problem is linearly separable if you can solve it with a single straight line (or, in higher dimensions, a flat surface). AND and OR are linearly separable. XOR is not.

<DecisionBoundaryExplorerWidget />

<TryIt>
Drag the line to solve AND — isolate the (1,1) corner from everything else. Then solve OR. Then try XOR: put (0,1) and (1,0) together while keeping (0,0) and (1,1) apart. No matter how you rotate or shift the line, you can't get all four right. Scroll to rotate, drag to move. XOR is fundamentally not a straight-line problem.
</TryIt>

<Callout type="info" title="The XOR Crisis">
In 1969, Minsky and Papert published *Perceptrons*, proving that single-layer networks can't solve XOR. This was taken as proof that neural networks were a dead end. It killed most neural network research for over a decade — the "AI winter." The fix was always sitting there, obvious in hindsight: use more than one layer.
</Callout>

## Breaking Through

If one neuron draws one line, what happens when you use *two* neurons and combine their outputs?

Two lines. And with two lines, you can carve out regions that no single line could isolate.

Here's one way to build XOR from neurons:
- Neuron A computes OR: "is *at least one* input on?"
- Neuron B computes NAND (not-and): "are they *not both* on?"
- Output neuron computes AND of A and B: "at least one is on, but not both."

That's XOR. Three neurons, two layers — problem solved.

This is the fundamental insight about depth: **each layer transforms the data into a new representation where the next layer's job is easier.** The hidden layer doesn't solve the problem directly. It reshapes it into a problem that a single line *can* solve.

<XORBreakthroughWidget />

<TryIt>
Switch from 1 layer to 2 layers. Watch the right panel — it shows what the hidden layer "sees." The four input points have been *moved*: in this new space, the two XOR-true cases are on one side and the two XOR-false cases are on the other. The hidden layer transformed an impossible problem into an easy one.
</TryIt>

<KeyInsight>
**Depth transforms problems.** A single neuron draws one line. A hidden layer transforms the entire input space — stretching, folding, rearranging the points — so that what was previously inseparable becomes separable. Each layer makes the next layer's job easier. This is why neural networks are built in layers.
</KeyInsight>

## The Activation Trick

You might be thinking: great, so more layers = more power. Let's just stack a hundred layers and solve everything!

But there's a catch — and it's the most important thing in this chapter.

What happens if you remove the activation function? A neuron without an activation function is just: `output = wA x A + wB x B + bias`. That's a **linear** function — it just multiplies and adds. And here's the thing about linear functions: **a linear function of a linear function is still a linear function.**

Layer 1 takes inputs and multiplies and adds. Layer 2 takes those results and multiplies and adds. But the whole thing simplifies to... one multiplication and one addition. You could collapse a hundred layers into a single layer with different numbers. No matter how deep you go, you're still drawing a straight line.

It's like multiplying a number by 3, then by 5, then by 2. You could have just multiplied by 30. Adding more multiplications never gets you anything beyond one multiplication.

The activation function *breaks* this. Because it bends the curve — it's not a straight line — two layers with activation are genuinely more powerful than one. This nonlinearity is what makes depth meaningful. Remove it, and no matter how elaborate your architecture looks, you're still stuck with a single straight line.

<LinearCollapseDemoWidget />

<TryIt>
Turn off the activation function and set depth to 5 layers. Hit Train. No matter how long it runs, the boundary stays straight — five layers of nothing is still nothing. Now turn activation back on, keep 5 layers, hit Train. Watch the boundary bend and curve around the data. The activation function isn't decoration. It's the entire reason depth works.
</TryIt>

<KeyInsight>
**Without activation functions, depth is an illusion.** A stack of linear layers always collapses to a single linear layer — the decision boundary is a straight line no matter how deep you go. The nonlinear activation function is what makes each layer *genuinely add* to the network's expressive power.
</KeyInsight>

## What Can Neurons Compute?

Neurons with activation functions, stacked in layers, can curve the decision boundary. But *how much* can they curve it? What can they actually compute?

The answer, surprisingly, is: **anything**. This is the **universal approximation theorem** — a neural network with enough neurons can approximate any continuous function to any desired accuracy. In theory, even a single hidden layer suffices. In practice, "enough neurons" might mean millions. Adding more layers lets you do the same job with far fewer neurons — depth is exponentially more efficient than width.

Let's see this in action. Below, you can place points on a canvas — blue and orange — and watch a neural network learn to classify them in real time. Draw any pattern you like: a circle inside a ring, a zigzag, a smiley face. Then see how the architecture affects what the network can learn.

<NeuralNetworkTrainerWidget />

<TryIt>
Load the "circle" preset — blue points surrounded by orange. With 1 layer, the boundary can't wrap around the circle. Switch to 2 layers and watch it learn the ring shape. Now try "spirals" — that's the classic hard problem. You'll need at least 3 layers and more neurons. Try drawing your own pattern and experimenting with the architecture controls.
</TryIt>

<Callout type="info" title="Regression vs. Classification">
The point-classification mode above predicts *categories* — blue or orange. That's **classification**. But the same neural network architecture can also predict *numbers* — how high is a curve at this point? That's **regression**. Same architecture, same training algorithm. The only difference is how you interpret the output.
</Callout>

## Not All Activations Are Equal

We've been using sigmoid as our activation function. But there are other options, and the choice matters a lot for deep networks.

The problem shows up during training. As we saw in Chapter 2, training works by figuring out which direction to adjust each weight to reduce the error. In deep networks, this signal has to travel backward through every layer. And some activation functions make this signal fade away as it passes through.

Sigmoid has flat regions near 0 and 1 — once you're deep in the flat part, the signal for "which way to adjust" is nearly zero. Stack several layers of this, and the learning signal evaporates before it reaches the early layers. This is called the **vanishing gradient problem**.

**ReLU** — `max(0, x)` — fixes this for positive inputs. It's the simplest possible activation: just cut off anything below zero. For positive values, the signal passes through at full strength. But if a neuron's input goes permanently negative, it outputs 0 and can never recover. It's a **dead neuron**.

**Swish** — `x x sigmoid(x)` — is a modern compromise that avoids dead neurons while keeping signal flowing.

<ActivationFunctionExplorerWidget />

<TryIt>
Set depth to 8 layers with sigmoid. Hit Train and watch the gradient heatmap — the early layers (top rows) are dark, meaning the learning signal has faded to nearly zero. They're barely learning. Switch to ReLU: the signal flows better. Switch to Swish: signals flow *and* fewer neurons die. This is why modern networks rarely use sigmoid as their activation.
</TryIt>

# Vectors

<Lead>
The word "vector" sounds intimidating. It isn't. A vector is just a list of numbers.
But this simple idea turns out to be the key to understanding what neurons actually do — and why stacking them in layers works at all.
</Lead>

## Describing Things with Vectors

You already use lists of numbers to describe things. Your position on the planet is two numbers: (latitude, longitude). A color on screen is three numbers: (red, green, blue). An RPG character has stats like (strength, magic, speed, defense). Each of these is a **vector** — a list of numbers that together describe something.

We can use the same idea to describe almost anything. Take animals: rate each one from 0 to 1 on properties like big, scary, hairy, cuddly, fast, and fat, and you get a vector that captures what makes that animal distinctive.

<VectorPropertyExplorerWidget>
Switch between tabs to see different kinds of vectors. Click two items to compare their vectors side by side — green bars mean the values are similar, red bars mean they're different. Which pair is the most similar? Try all four tabs to see how the same idea applies to completely different things.
</VectorPropertyExplorerWidget>

The number of measurements is the vector's **dimension** — our animal vectors have 6 dimensions, one per property. More dimensions let you make finer distinctions. With just "big" and "scary," a bear and a dog might look similar. Add "cuddly" and they separate out.

Notice something important: the vector captures what we *chose* to measure. If we'd picked different properties, the same items would get different vectors. **The choice of dimensions determines what the vector can distinguish.**

<KeyInsight>
**A vector is a compact description.** It's a list of numbers that together describe something. More dimensions mean finer distinctions. Vectors can describe positions, colors, characters, animals, foods — anything you can capture with numbers.
</KeyInsight>

## Unit Vectors

The vectors above have been scaled so that the sum of the squares of all their dimensions adds up to 1. A vector like this is called a **unit vector**. You can think of it as the generalization of the number 1 to multiple dimensions — just as 1 is the "standard size" for a number, a unit vector is the "standard size" for a vector.

This scaling preserves the *proportions* between dimensions — a bear is still scarier than it is fast — but puts every vector on the same footing, which turns out to be important for measuring similarity.

## Cosine Similarity

Now that we can describe things as unit vectors, how do we measure **how similar** two things are? There's a beautifully simple operation called **cosine similarity**. You multiply each matching pair of numbers, then add up all the results:

**similarity = (a₁ × b₁) + (a₂ × b₂) + … + (aₙ × bₙ)**

The result is a single number:
- **1** means they're identical
- **0** means they have nothing in common
- **−1** means they're completely opposite

This works for unit vectors, because they have been scaled so that their cosine similarity with themself is one. See the next section to find out what happens if you do the same math with vectors that aren't unit vectors.

<DotProductComparisonWidget>
Pick two items and compare their similarity. The multiply column shows what happens at each dimension — when both items score high on the same property, that contributes a lot to the total. When one scores high and the other low, it contributes almost nothing. Try the **2D Arrows** tab to see how similarity relates to the angle between vectors.
</DotProductComparisonWidget>

Why does multiply-and-add measure similarity? Look at the per-dimension math. When both vectors score high on the same property, multiplying those gives a big number — that shared trait contributes a lot to the total. But if one scores high and the other low, multiplying gives almost nothing. Add up all these products, and you get a total that's high when the vectors agree and low when they don't.

## The Dot Product

Cosine similarity is actually a special case of a more general operation called the **dot product**. The math is exactly the same — multiply matching pairs, add them up — but the dot product works on vectors of *any* length, not just unit vectors.

With regular numbers, you can multiply them: 3 × 5 = 15. The dot product extends multiplication to multiple dimensions:

**a · b = (a₁ × b₁) + (a₂ × b₂) + … + (aₙ × bₙ)**

If your vectors have just one dimension, this is ordinary multiplication. With more dimensions, something interesting happens. Like multiplying regular numbers, the dot product gets bigger when each input is bigger. But unlike regular numbers, it also depends on **direction** — it gets bigger when the vectors point the same way, and goes negative when they point in opposite directions. If they're at right angles, it's zero.

<DotProduct2DWidget>
Drag the two vectors around. Watch the step-by-step math on the right — each matching pair of numbers gets multiplied, then the products are added up. The dot product is big when both vectors are long and point the same way. It's zero when they're at right angles. It goes negative when they point in opposite directions.
</DotProduct2DWidget>

<KeyInsight>
**The dot product extends multiplication to vectors.** Like regular multiplication, it gets bigger when the inputs are bigger. But it also captures direction: same direction → large positive, perpendicular → zero, opposite → large negative. Cosine similarity is a dot product between unit vectors, giving a pure similarity score. The dot product is the foundation of how neural networks process information.
</KeyInsight>

## A Neuron Is (Mostly) a Dot Product

Here's the connection that ties everything together. Remember from the previous chapter that a neuron computes a **weighted sum** of its inputs, adds a bias, then applies an activation function. That weighted sum is a dot product — the neuron has a **weight vector** and receives an **input vector**:

<Callout>
**output = activation( w · x + bias )**

where **w** = (w₁, w₂, …, wₙ) is the weight vector and **x** = (x₁, x₂, …, xₙ) is the input vector.
</Callout>

The neuron is asking **"is this input what I'm looking for?"**.

* **The direction** of the weight vector determines what kind of thing the neuron is looking for.
* **The magnitude** of the weight vector determines how much of what it's looking for the neuron needs to see in order to get excited.
* **The bias** determines how excited it is if it hasn't seen anything at all.

This is all a bit brain bending, but makes more sense if you play with it. Try the **Animals** tab and set the weight to "Bear" — now you have a bear detector! Feed it different animals as input and watch the output. A dog gets a high score because it shares many bear-like properties. A mouse scores low. You can do the same with any category — set the weight to "Pizza" on the **Foods** tab and you have a pizza detector.

The **weight magnitude** slider controls how big the weight vector is. A large magnitude means the neuron is very decisive — it fires strongly for good matches and stays very quiet for bad ones. A small magnitude means the neuron is wishy-washy — it gives a middling output for everything.

<NeuronDotProductWidget>
Drag the weight and input vectors. Watch the angle between them — when the vectors align (small angle), the output is high. When they're perpendicular (90°), the output depends mostly on the bias. When they oppose each other (large angle), the output drops. Try the other tabs to build a bear detector or a pizza detector. Try making the weight magnitude large or small to see how it affects the neuron's decisiveness. This visualization shows two dimensions, but real neurons have weight and input vectors with hundreds or thousands of dimensions — the same geometric intuition applies.
</NeuronDotProductWidget>

<KeyInsight>
**A neuron is a direction detector.** It computes the dot product of its input vector with its weight vector, adds a bias to shift the threshold, and squashes the result through an activation function. The weights define a direction in input space, and the neuron fires when the input points that way.
</KeyInsight>

## What's Next

A vector is just a list of numbers — but it's a remarkably powerful idea. We've seen how cosine similarity measures how alike two vectors are, how the dot product generalizes multiplication to multiple dimensions, and how a neuron uses the dot product to detect patterns in its input.

But so far we've been describing things with *hand-picked* properties — big, scary, hairy. That works for animals, because animals are one kind of thing and we can choose sensible properties for them. But what about words? "Dog" and "democracy" and "purple" and "running" — there's no single set of properties that makes sense for all of them. "How scary is the color blue?" isn't a meaningful question.

In the next chapter, we'll see how AI solves this problem: instead of choosing dimensions by hand, it *learns* vectors where similar words end up nearby and directions encode meaningful relationships — all without any human labeling.

<TryItInPyTorch notebook="vectors">
Create vectors in PyTorch, compute dot products and cosine similarity, and build a neuron as a dot product.
</TryItInPyTorch>

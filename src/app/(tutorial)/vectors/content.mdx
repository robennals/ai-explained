# Vectors

<Lead>
The word "vector" sounds intimidating. It isn't. A vector is just a list of numbers.
But this simple idea turns out to be the key to understanding what neurons actually do — and why stacking them in layers works at all.
</Lead>

## Describing Things with Vectors

You already use lists of numbers to describe things. Your position on the planet is two numbers: (latitude, longitude). A color on screen is three numbers: (red, green, blue). An RPG character has stats like (strength, magic, speed, defense). Each of these is a **vector** — a list of numbers that together describe something.

We can use the same idea to describe almost anything. Take animals: rate each one from 0 to 1 on properties like big, scary, hairy, cuddly, fast, and fat, and you get a vector that captures what makes that animal distinctive.

<VectorPropertyExplorerWidget>
Switch between tabs to see different kinds of vectors. Click two items to compare their vectors side by side — green bars mean the values are similar, red bars mean they're different. Which pair is the most similar? Try all four tabs to see how the same idea applies to completely different things.
</VectorPropertyExplorerWidget>

The number of measurements is the vector's **dimension** — our animal vectors have 6 dimensions, one per property. More dimensions let you make finer distinctions. With just "big" and "scary," a bear and a dog might look similar. Add "cuddly" and they separate out.

Notice something important: the vector captures what we *chose* to measure. If we'd picked different properties, the same items would get different vectors. **The choice of dimensions determines what the vector can distinguish.**

Vectors can also describe things in 2D space. A velocity is two numbers — how fast something is going east/west, and how fast it's going north/south. Together, these two numbers define a direction and a speed. The vectors we care about in AI usually have many more dimensions than two, but 2D vectors are easy to draw and interact with, so we'll use them a lot in this chapter to illustrate how vectors work.

<VelocityExplorerWidget>
Drag the arrow to change the velocity. The card on the right shows the same information as a list of two numbers. A long arrow means high speed; the arrow's direction shows which way. The two numbers *are* the velocity — the arrow is just a way to picture them.
</VelocityExplorerWidget>

<KeyInsight>
**A vector is a compact description.** It's a list of numbers that together describe something. More dimensions mean finer distinctions. Vectors can describe positions, colors, characters, velocities — anything you can capture with numbers.
</KeyInsight>

## The Dot Product

Now for the most important operation in AI.

With regular numbers, you can multiply them together: 3 × 5 = 15. The **dot product** extends multiplication to multiple dimensions. You multiply each matching pair of numbers, then add up all the results:

**a · b = (a₁ × b₁) + (a₂ × b₂) + … + (aₙ × bₙ)**

If your vectors have just one dimension, this is ordinary multiplication. With more dimensions, something interesting happens. Like multiplying regular numbers, the dot product gets bigger when each input is bigger. But unlike regular numbers, it also depends on **direction** — it gets bigger when the vectors point the same way, and goes negative when they point in opposite directions. If they're at right angles, it's zero.

Let's play with this and see what it feels like in two dimensions:

<DotProduct2DWidget>
Drag the two vectors around. Watch the step-by-step math on the right — each matching pair of numbers gets multiplied, then the products are added up. The dot product is big when both vectors are long and point the same way. It's zero when they're at right angles. It goes negative when they point in opposite directions.
</DotProduct2DWidget>

Why does multiply-and-add capture direction? Look at the step-by-step math. When both vectors have a large x value, multiplying those gives a big number — that shared component contributes a lot to the total. But if one vector has a large x and the other has a small x, multiplying gives almost nothing. Add up all these products, and you get a total that's high when the vectors agree and low when they don't.

<KeyInsight>
**The dot product extends multiplication to vectors.** Like regular multiplication, it gets bigger when the inputs are bigger. But it also captures direction: same direction → large positive, perpendicular → zero, opposite → large negative. This single operation is the foundation of how neural networks process information.
</KeyInsight>

## Unit Vectors and Similarity

Because the dot product depends on both direction *and* length, it doesn't give us a pure similarity score — two long vectors that vaguely point the same way can have a bigger dot product than two short vectors that point exactly the same way. To get *pure* similarity, ignoring size, we use **unit vectors** — vectors that have been scaled so their length is 1.

The dot product of two unit vectors gives a pure similarity score:
- **1** means they're identical
- **0** means they have nothing in common
- **−1** means they're completely opposite

In AI, this is called **cosine similarity** — you'll see the term a lot.

<DotProductComparisonWidget>
On the **2D Arrows** tab, drag the arrows around the circle. Since they're constrained to the circle, they're always unit vectors, so the dot product is pure similarity. Then switch to the other tabs — each item is a unit vector, so you can compare pure similarity across animals, RPG characters, foods, and instruments. Look at the per-property multiplication to see which shared traits contribute the most.
</DotProductComparisonWidget>

## A Neuron Is (Mostly) a Dot Product

Here's the connection that ties everything together. Remember from the previous chapter that a neuron computes a **weighted sum** of its inputs, adds a bias, then applies an activation function. That weighted sum is a dot product — the neuron has a **weight vector** and receives an **input vector**:

<Callout>
**output = activation( w · x + bias )**

where **w** = (w₁, w₂, …, wₙ) is the weight vector and **x** = (x₁, x₂, …, xₙ) is the input vector.
</Callout>

The neuron is asking **"is this input what I'm looking for?"**.

* **The direction** of the weight vector determines what kind of thing the neuron is looking for.
* **The magnitude** of the weight vector determines how much of what it's looking for the neuron needs to see in order to get excited.
* **The bias** determines how excited it is if it hasn't seen anything at all.

This is all a bit brain bending, but makes more sense if you play with it. Try the **Animals** tab and set the weight to "Bear" — now you have a bear detector! Feed it different animals as input and watch the output. A wolf gets a high score because it shares many bear-like properties. A goldfish scores low. You can do the same with any category — set the weight to "Pizza" on the **Foods** tab and you have a pizza detector.

The **weight magnitude** slider controls how big the weight vector is. A large magnitude means the neuron is very decisive — it fires strongly for good matches and stays very quiet for bad ones. A small magnitude means the neuron is wishy-washy — it gives a middling output for everything.

<NeuronDotProductWidget>
Drag the weight and input vectors. Watch the angle between them — when the vectors align (small angle), the output is high. When they're perpendicular (90°), the output depends mostly on the bias. When they oppose each other (large angle), the output drops. Try the other tabs to build a bear detector or a pizza detector. Try making the weight magnitude large or small to see how it affects the neuron's decisiveness. This visualization shows two dimensions, but real neurons have weight and input vectors with hundreds or thousands of dimensions — the same geometric intuition applies.
</NeuronDotProductWidget>

<KeyInsight>
**A neuron is a direction detector.** It computes the dot product of its input vector with its weight vector, adds a bias to shift the threshold, and squashes the result through an activation function. The weights define a direction in input space, and the neuron fires when the input points that way.
</KeyInsight>

## What's Next

A vector is just a list of numbers — but it's a remarkably powerful idea. We've seen how the dot product measures similarity, how unit vectors give us pure similarity scores, and how a neuron uses the dot product to detect patterns in its input.

But so far we've been describing things with *hand-picked* properties — big, scary, hairy. That works for animals, because animals are one kind of thing and we can choose sensible properties for them. But what about words? "Dog" and "democracy" and "purple" and "running" — there's no single set of properties that makes sense for all of them. "How scary is the color blue?" isn't a meaningful question.

In the next chapter, we'll see how AI solves this problem: instead of choosing dimensions by hand, it *learns* vectors where similar words end up nearby and directions encode meaningful relationships — all without any human labeling.

<TryItInPyTorch notebook="vectors">
Create vectors in PyTorch, compute dot products and cosine similarity, and build a neuron as a dot product.
</TryItInPyTorch>

{/* Backup of sections/widgets removed from vectors chapter — to be covered in a later chapter. */}

{/* --- Velocity / 2D space section --- */}

Vectors can also describe things in 2D space. A velocity is two numbers — how fast something is going east/west, and how fast it's going north/south. Together, these two numbers define a direction and a speed. The vectors we care about in AI usually have many more dimensions than two, but 2D vectors are easy to draw and interact with, so we'll use them a lot in this chapter to illustrate how vectors work.

<VelocityExplorerWidget>
Drag the arrow to change the velocity. The card on the right shows the same information as a list of two numbers. A long arrow means high speed; the arrow's direction shows which way. The two numbers *are* the velocity — the arrow is just a way to picture them.
</VelocityExplorerWidget>

{/* --- "A Neuron Draws a Line" (version from current chapter) --- */}

## A Neuron Draws a Line

If a neuron fires when its input aligns with its weights, what does that look like across *all possible inputs*?

The heatmap below shows the neuron's output for every point in 2D input space. Green means the neuron fires (output near 1), red means it doesn't (output near 0). The dashed white line is the **decision boundary** — where the output is exactly 0.5.

<NeuronDecisionBoundaryWidget>
Drag the weight vector (orange) to point in different directions — watch the decision boundary rotate to stay perpendicular to it. Drag the input vector (blue) around and watch its output match the heatmap. Now adjust the bias: positive bias pushes the boundary away from the weight vector (more green), negative bias pushes it toward the weight vector (more red). Notice that the weight vector's *direction* controls the angle of the line, while the bias controls where the line sits.
</NeuronDecisionBoundaryWidget>

The decision boundary is always a straight line, perpendicular to the weight vector. On the side the weight vector points toward, inputs align with the weights and the neuron fires. On the other side, inputs oppose the weights and the neuron stays quiet. The bias shifts this line toward or away from the origin — it controls *how much* alignment the neuron demands before it fires.

{/* --- Old version of "A Neuron Draws a Line" below --- */}

{/* --- Dot Product Explorer (Component/Projection view) --- */}

<DotProductExplorerWidget>
Drag the arrow tips. In Component view, watch the individual products and their sum. Switch to Projection view to see the angle and the projection of one vector onto the other. Both views always show the same number.
</DotProductExplorerWidget>

## A Neuron Draws a Line

If a neuron fires when its input aligns with its weights, what does that look like geometrically?

Plot every possible input as a point in 2D space. The neuron's weighted sum — w₁ × x₁ + w₂ × x₂ + bias — defines a straight line through this space. On one side of the line, the output is close to 1. On the other side, close to 0. The line itself is the **decision boundary** — where the neuron is exactly 50/50.

<DecisionBoundaryExplorerWidget>
Click AND to see how the weight vector and bias draw a line that isolates the (1,1) corner. Click OR to see the line shift. Drag the weight vector or adjust the bias to see how they move the boundary. The blue input vector shows the output at any position — notice it always matches the heatmap. Now try XOR: no matter how you move the line, you can't get all four corners right.
</DecisionBoundaryExplorerWidget>

This is called **linear separability**: a problem is linearly separable if you can solve it with a single straight line (or, in higher dimensions, a flat surface). AND and OR are linearly separable. XOR is not — the "true" corners sit on opposite diagonals, and no single straight line can separate them from the "false" corners.

## Two Lines Solve XOR

If one neuron draws one line, what happens with *two* neurons? Two lines. And two lines can carve out regions that one line never could.

Each hidden neuron transforms the input into its own coordinate. In this new space, points that were on opposite corners get moved to the same side — what was inseparable becomes separable.

<XORBreakthroughWidget>
Switch from 1 layer to 2 layers. Watch the right panel — it shows what the hidden layer "sees." The four input points have been *moved*: in this new space, the two XOR-true cases are on one side and the two XOR-false cases are on the other. The hidden layer transformed an impossible problem into an easy one.
</XORBreakthroughWidget>

This is the fundamental insight about depth: **each layer transforms the data into a new representation where the next layer's job is easier.** The hidden layer doesn't solve the problem directly — it reshapes it into a problem that a single line *can* solve.

<KeyInsight>
**Depth transforms problems.** A single neuron draws one line. A hidden layer transforms the entire input space — stretching, folding, rearranging — so that what was inseparable becomes separable. Each layer makes the next layer's job easier.
</KeyInsight>

## Why Activation Functions Matter

If adding layers is so powerful, why not just stack a hundred? There's a catch — and it's the most important idea in this chapter.

Without the activation function, a neuron is just: output = w₁ × x₁ + w₂ × x₂ + bias. That's a **linear** function — just multiplication and addition. And here's the thing: **a linear function of a linear function is still a linear function.**

Layer 1 multiplies and adds. Layer 2 multiplies and adds again. But the whole thing simplifies to one multiplication and one addition. You could collapse a hundred layers into a single layer with different numbers. No matter how deep you go, you're still drawing one straight line.

It's like multiplying a number by 3, then by 5, then by 2. You could have just multiplied by 30.

The activation function *breaks* this. Because it bends the curve — it's not a straight line — two layers with activation are genuinely more powerful than one. This nonlinearity is what makes depth meaningful.

<LinearCollapseDemoWidget>
Turn off the activation function and set depth to 5 layers. Hit Train. No matter how long it runs, the boundary stays a straight line — five layers of nothing is still nothing. Now turn activation back on, keep 5 layers, hit Train. Watch the boundary bend and curve around the data. The activation function is the entire reason depth works.
</LinearCollapseDemoWidget>

<KeyInsight>
**Without activation functions, depth is an illusion.** A stack of linear layers always collapses to a single linear layer — the decision boundary is a straight line no matter how deep you go. The nonlinear activation is what makes each layer genuinely add to the network's expressive power.
</KeyInsight>

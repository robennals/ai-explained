# Matrix Math and Linear Transformations

<Lead>
Every layer in a neural network takes numbers in, multiplies and adds, and puts numbers out. That operation has a name — matrix multiplication — and a beautiful geometric meaning. Each layer is a transformation of space itself.
</Lead>

## Starting Simple: One Dimension

Before we tackle the full complexity of matrices, let's start with the simplest possible case: one dimension.

In one dimension, a "matrix" is just a single number. You multiply every input by that number. Multiply by 2, and everything stretches away from zero. Multiply by 0.5, and everything contracts toward zero. Multiply by &minus;1, and everything flips to the other side.

<Transform1DWidget />

<TryIt>
Drag the slider to different values. What happens when you multiply by 0? By &minus;1? By a number between 0 and 1? Notice that zero itself never moves, no matter what you multiply by — it's a **fixed point** of every linear transformation.
</TryIt>

Here's a simple but important idea: in 1D, the number "1" is a **basis vector** — a building block. Every other number is just some multiple of 1. When you multiply by 2, you're saying "take the basis vector 1, and stretch it to 2." That single number tells you where the basis vector lands after the transformation, and *everything else follows automatically*.

In 1D, there's not much a "matrix" can do: stretch, shrink, flip, or collapse to a point. One number, one effect. Let's add a dimension.

## Two Dimensions: Where It Gets Interesting

In 1D we had one basis vector. In 2D we have **two**: one pointing right along the x-axis, and one pointing up along the y-axis. Every point in 2D is some combination of "go right" and "go up" — it's built from those two building blocks.

A 2&times;2 matrix tells you where each basis vector lands. That takes **four numbers**, arranged in a grid:

<div className="my-6 flex justify-center">
<div className="inline-block rounded-lg border border-border bg-surface px-6 py-3 font-mono text-lg">
<div className="flex items-center gap-1"><span className="text-xl text-muted">[</span> <span className="w-8 text-center" style={{color: "#ef4444"}}>a</span> <span className="w-8 text-center" style={{color: "#22c55e"}}>b</span> <span className="text-xl text-muted">]</span></div>
<div className="flex items-center gap-1"><span className="text-xl text-muted">[</span> <span className="w-8 text-center" style={{color: "#ef4444"}}>c</span> <span className="w-8 text-center" style={{color: "#22c55e"}}>d</span> <span className="text-xl text-muted">]</span></div>
</div>
</div>

The <span style={{color: "#ef4444"}}>first column</span> (a, c) tells you where the x-axis basis vector ends up. The <span style={{color: "#22c55e"}}>second column</span> (b, d) tells you where the y-axis basis vector ends up. Everything else — every single point in the entire plane — just follows along.

Four numbers. That's it. But watch what they can do:

<Transform2DWidget />

<TryIt>
Switch to the **Cat** and try the presets. Hit **Rotate 90&deg;** — the cat spins a quarter turn. **Shear** — the cat leans over like it's dodging something. **Reflect** — mirror-image cat! **Collapse** — the cat gets squished into a line (poor cat). Now switch to the **Mona Lisa** and try **Rotate 45&deg;** — can you tell she's tilted? Try making your own transformation: can you make the Mona Lisa twice as wide but the same height? (Hint: drag *a* to 2 — you're stretching the x-axis basis vector to twice its length.)
</TryIt>

<KeyInsight>
A 2&times;2 matrix is completely defined by four numbers. Those four numbers control every possible 2D linear transformation: rotation, scaling, shearing, reflection, and collapse. The first column tells you where the x-axis basis vector goes. The second tells you where the y-axis basis vector goes. Everything else follows.
</KeyInsight>

## Another Way to See It: New Axes

There's another way to read a matrix — by looking at the **rows** instead of the columns.

The <span style={{color: "#ef4444"}}>first row</span> of the matrix defines where the x-axis of the **new** space points in the **old** space. The <span style={{color: "#22c55e"}}>second row</span> defines where the y-axis of the new space points in the old space. In other words, the rows tell you the shape of the new coordinate grid.

In the widget below, we've drawn colored lines called **iso-lines** that show how the grid of the new space looks when projected back onto the old space. On the left, you can see these iso-lines overlaid on the original space: <span style={{color: "#ef4444"}}>red lines</span> show the new x-axis grid, and <span style={{color: "#22c55e"}}>green lines</span> show the new y-axis grid. On the right, you see the new space with a regular grid.

This is kind of hard to explain in words, but if you play with it, it should start to make sense. Try lining up the iso-lines with a particular point in the input space (on the left) and notice that the equivalent lines in the output space (on the right) line up with the same point.

<BasisVectorViewWidget />

<TryIt>
Slide the sliders around and see how the iso-lines change shape. Try the presets too — **Rotate 90&deg;**, **Shear**, **Collapse** — and watch what happens to the grid.
</TryIt>

Both ways of reading a matrix — columns ("where do the old axes land?") and rows ("what do the new axes look like?") — describe the same transformation. In neural networks, the row view is especially useful: each output neuron uses one row of the weight matrix to measure the input.

## Changing Dimensions

So far our matrices have been square — 2&times;2 matrices mapping 2D to 2D. But matrices don't have to be square. A **1&times;2 matrix** takes a 2D point and produces a single number. It *projects* two dimensions down to one.

This is dimension reduction — and it's everywhere in neural networks. A layer with 768 inputs and 64 outputs is a 64&times;768 matrix. It takes a high-dimensional representation and squishes it down to a smaller one, keeping (hopefully) the important information and discarding the rest.

Below, you can see 2D points being projected onto a line. Each colored point has a letter so you can track where it lands. Rotate the line and watch the points slide along it.

<DimensionProjectionWidget />

<TryIt>
Start with the **Arrow** shape. At **0&deg;** (horizontal line), you're only keeping each point's x-coordinate — the arrow gets squished onto the x-axis. Switch to **90&deg;** (vertical) and you keep only the y-coordinate. Now try **45&deg;** — you get a mix of both.

Notice how some directions preserve the shape's structure better than others. At 0&deg;, points B and C land in almost the same spot — they're "confused." At 45&deg;, all the points spread out more evenly. Finding the direction that preserves the most information is exactly what algorithms like PCA (principal component analysis) do.
</TryIt>

When the matrix has **more rows than columns**, it does the opposite — it *embeds* a lower dimension into a higher one. A 768&times;1 matrix takes a single number and places it into a 768-dimensional space. Neural networks do both: expanding dimensions to create richer representations, then compressing them back down.

## The Neural Network Connection

Here's the punchline. A neural network layer takes some inputs, multiplies each one by a **weight**, and adds up the results. That's it — it's just multiplying by a matrix.

Say we have two inputs and two outputs. We need four weights — let's call them <span style={{color: "#ef4444"}}>**a**</span>, <span style={{color: "#22c55e"}}>**b**</span>, <span style={{color: "#3b82f6"}}>**c**</span>, and <span style={{color: "#a855f7"}}>**d**</span>:

- **output 1** = <span style={{color: "#ef4444"}}>a</span> &times; input 1 + <span style={{color: "#22c55e"}}>b</span> &times; input 2
- **output 2** = <span style={{color: "#3b82f6"}}>c</span> &times; input 1 + <span style={{color: "#a855f7"}}>d</span> &times; input 2

Those four weights are exactly the four numbers in a matrix! The neuron diagram and the matrix are just two ways of writing the same thing — same numbers, same colors, same math.

<NeuronVsMatrixWidget />

<TryIt>
Adjust the sliders and watch everything update together — the neuron diagram, the matrix, and the geometric view are all showing the **exact same math**. Try **Rotate 90&deg;** and watch the arrow spin. Try **Collapse to line** and watch the arrow get squished flat.
</TryIt>

This is a big idea:

- In the **neuron view**, we think: *"Each output adds up the inputs, multiplied by weights."*
- In the **geometric view**, we think: *"The weights rotate and stretch the space."*

Both are true. They're just two ways of seeing the same thing.

<KeyInsight>
A neural network layer and a matrix transformation are the same thing, drawn differently. When we train a neural network, we're learning the right matrix at each layer — the right geometric transformation to apply to the data. Real neural networks work in hundreds or thousands of dimensions — a single layer in a typical language model transforms a 768-dimensional space, with over half a million matrix entries — but the math is identical to our 2D examples. Just... bigger.
</KeyInsight>

## Training Is Geometry

When you train a neural network, you're searching for the right sequence of geometric transformations. Each layer rotates, stretches, and (with activation functions) bends the input space, gradually reshaping it until the data is arranged in a way that makes the answer obvious.

Imagine data points from two classes tangled together in a spiral. No single straight line can separate them. But a matrix transformation can stretch and rotate the spiral, and an activation function can fold it. Stack enough of these transform-and-fold operations, and you can untangle any pattern into neatly separated clusters.

That's what training discovers: the right sequence of rotations, stretches, and folds to make the problem simple.

## What's Next

We've seen that every neural network layer is a matrix transformation — a geometric reshaping of space. But how does the network actually *find* the right matrices? In the next chapter, we'll see how gradient descent rolls downhill through a landscape with millions of dimensions, automatically discovering the transformations that untangle data.

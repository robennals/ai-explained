# Embeddings and Vector Spaces

<Lead>
How do you turn a word into something a neural network can work with? You turn it into a list of numbers — but not just any numbers. The right numbers encode *meaning itself* as geometry.
</Lead>

## From True/False to Words

In the previous chapter, we worked with logic values — things that are either true or false. But GPT doesn't work with true/false values. It works with *words*. And words are messy.

One idea: give each word its own input neuron. There's a "dog" neuron, a "cat" neuron, a "magnificent" neuron. When the input is "dog", the dog neuron fires and everything else is zero. But English has over 170,000 words — that's 170,000 input neurons, and the network has to independently learn what every single one means. It gets no hint that "dog" and "puppy" are related, or that "big" and "enormous" mean the same thing.

OK, so what if we just assign each word a single number instead? "Dog" = 1, "cat" = 2, "car" = 3, "puppy" = 4. Now we only need one input neuron. But the numbers are arbitrary — "dog" being 1 and "puppy" being 4 doesn't mean dogs are less than puppies. The network can't make sense of these numbers because they don't *mean* anything. Moving from 1 to 2 takes you from "dog" to "cat", but moving from 2 to 3 takes you from "cat" to "car" — there's no pattern to learn.

What if we could make the numbers actually *mean* something?

## Words on a Line

What if we gave each word a number that captures something about what it means? For example, we could score every word by how *big* it is. An ant gets a low number. An elephant gets a high number. A truck is up there too.

<WordNumberLineWidget />

<TryIt>
Toggle between the three axes. On "Size", notice how elephant ends up next to truck — they're both big, but have nothing else in common. Switch to "Alive" — now animals separate from objects, but elephant and mouse are neighbors. One number captures ONE property but conflates everything else.
</TryIt>

The problem is clear: one dimension isn't enough. No matter which property you choose, words that are different in other ways end up as neighbors. Elephant is next to truck on the size axis. Mouse is next to coin on the alive axis. A single number can only sort words along one axis of meaning.

## Adding a Second Dimension

What if we use *two* numbers? One for "how alive is it?" and one for "how big is it?" Now each word is a point on a 2D plane instead of a point on a line.

<Simple2DScatterWidget />

<TryIt>
Notice how categories naturally cluster. Animals form a group in the top-right (alive and various sizes). Vehicles cluster on the left (not alive). Elephant now separates from *both* mouse (different size) and truck (different aliveness).
</TryIt>

Two dimensions are dramatically more expressive than one. With just two numbers, we can separate categories that were hopelessly mixed on a single line. But we still can't distinguish everything — dog and cat overlap, guitar and violin are on top of each other. We'd need more dimensions: "is it a pet?", "does it have strings?", "is it edible?"

## Beyond Two Dimensions

Each new dimension adds discriminative power. With 5 dimensions, you could separate animals from vehicles from instruments from food. With 20 dimensions, you could tell a dog from a cat. With 50 or 100 dimensions, you could capture remarkably fine-grained distinctions.

But here's the problem: how do you decide *which* dimensions to use? "How dangerous is the color blue?" doesn't make sense. "How electronic is a cow?" There's no universal set of labeled dimensions that works for all words.

Real AI systems solve this differently. They don't use labeled dimensions at all. Instead, they learn a space where **similar words end up nearby** and **different words end up far apart**. The dimensions are just numbers — not "alive?" or "big?" — but the proximity structure captures meaning.

This is like the difference between organizing books by (genre, length, year) versus letting a librarian arrange them so that related books are on nearby shelves. The librarian's system has no labels, but it works better because it captures relationships that no fixed labeling scheme could.

<KeyInsight>
Real embeddings don't use labeled dimensions. They learn positions where similar things are nearby. This is more powerful because it can encode relationships that no fixed set of labels could capture.
</KeyInsight>

## Exploring a Real Embedding

Let's explore an actual word embedding — GloVe, trained by Stanford on 6 billion words of text. Each word is a point in 50-dimensional space. We can't visualize 50 dimensions, but we *can* ask: what's nearby?

<EmbeddingPlaygroundWidget />

<TryIt>
Start with "dog" — its neighbors are other animals. Try "guitar" — you get other instruments. Now try the analogy presets: "king" + close to "woman" and far from "man" gives you... *queen*. Try "puppy" + close to "cat" and far from "dog" — you get *kitten*. These analogies work because the embedding captures relationships between words as directions in space.
</TryIt>

The king/queen analogy is remarkable. It works because "the direction from man to woman" is consistent across the embedding space. King is to queen as man is to woman as boy is to girl. The embedding discovered this structure just from reading text — nobody told it about gender. It learned that these words appear in parallel patterns ("the king ruled" / "the queen ruled") and placed them accordingly.

<KeyInsight>
In a well-trained embedding, similar words are nearby *and* directions carry meaning. king - man + woman = queen. This arithmetic of meaning emerges from training on text, with no human labeling.
</KeyInsight>

## Directions Have Meaning

The king/queen analogy hints at something deeper: any two words define a *direction* in the embedding space, and you can project all words along that direction. The direction from "man" to "woman" separates words by gender. The direction from "small" to "big" separates words by size.

<CustomAxisScatterWidget />

<TryIt>
Try the "gender vs power" preset. See how king and prince cluster on one side while queen and princess cluster on the other? Try making your own axes — what direction does "fire" to "ice" define? Enter words in the search box to find them in the cloud of points.
</TryIt>

This is why unlabeled dimensions are so powerful. Even though no dimension is labeled "gender" or "size," those concepts exist as directions in the space. And there are far more meaningful directions than there could ever be labeled dimensions — every pair of words defines a potential axis of meaning.

## Where Do Embeddings Come From?

Two key ideas drive how embeddings are created:

**Learned from context.** Systems like Word2Vec and GloVe are trained on the principle that "a word is known by the company it keeps." Words that appear near the same other words get similar vectors. "Dog" and "cat" both appear near "pet", "fur", "veterinarian" — so they end up close together in the embedding.

**Learned for a task.** Inside a neural network, an embedding layer is simply a lookup table — a matrix with one row per word and one column per dimension. To get the embedding for a word, you look up its row. The matrix starts random and is shaped by backpropagation, just like any other set of weights. The network learns whatever representation helps it perform its task.

Modern language models use much higher dimensions than our 50-dimensional GloVe example: GPT-2 uses 768 dimensions, GPT-3 uses 12,288 dimensions. And they embed not individual words but *tokens* — subword pieces that let the model handle any text, including words it has never seen before.

<EmbeddingLayerDiagramWidget />

<TryIt>
Click different words and watch the highlighted row change. Toggle between "Before training" (random numbers) and "After training" (structured vectors from a real embedding). Before training, the vectors are meaningless noise. After training, similar words have similar patterns. The network learned this structure by adjusting the numbers to be useful for its task.
</TryIt>

## Beyond Words

Embeddings are one of the most powerful ideas in modern AI. The pattern — take something discrete, map it to a point in continuous space, let training shape the geometry — applies far beyond words:

- **Tokens** (subword pieces) in language models like GPT
- **Image patches** in vision models
- **Audio frames** in speech recognition
- **Users and products** in recommendation systems
- **Molecules** in drug discovery

In every case, the same principle holds: similar things end up nearby, and the geometry of the space encodes meaningful relationships. An embedding is a learned translation from symbols to geometry — and the geometry of meaning emerges from the pressure to be useful.

<KeyInsight>
An embedding is a learned mapping from discrete symbols to continuous geometry. The network discovers what dimensions to use and what structure to create — driven entirely by the need to perform its task well. This idea extends far beyond words to images, audio, molecules, and more.
</KeyInsight>

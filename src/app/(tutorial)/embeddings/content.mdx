# From Words to Meanings

<Lead>
In the last chapter, we described animals with hand-picked properties like size and scariness. That worked because animals are one kind of thing. But words represent *everything* — and there's no universal set of properties for all of them. AI solves this by letting the network learn a space where meaning is encoded in proximity and direction, not in any individual dimension.
</Lead>

## The Problem with Hand-Picked Dimensions

When we represented animals as vectors, we chose dimensions that made sense for animals: size, speed, danger. But what dimensions would you use for *all words*? "How big is the color blue?" doesn't make sense. "How fast is democracy?" There's no universal set of labeled dimensions that works for everything.

AI solves this by giving up on universal dimensions entirely. Instead of asking "what properties describe everything?", it places words with similar meanings close together — and lets the dimensions mean different things in different parts of the space. Near "dog," the dimensions might capture size and domestication; near "democracy," they might capture abstractness and political leaning.

This is a somewhat tricky idea, so let's build up to it starting with just a single dimension. What if we use different *regions* of the number line for different categories, and order items *within* each region by a different property?

<CombinedNumberLineWidget>
Try each preset. In "Animals × Foods", animals are on the left ordered by size, while foods are on the right ordered by sweetness. One number encodes the category AND a property within it. Try the other presets — each packs two categories with different orderings onto one line.
</CombinedNumberLineWidget>

This is surprisingly expressive! One number tells you both *what category* something belongs to (which region it's in) and *something about it* (its position within that region). But it still can't handle more than two categories well — we'd need more room on the line than we have.

## Adding a Second Dimension

What if we use *two* numbers? Now each word is a point on a 2D plane instead of a point on a line. This is dramatically more expressive — but still not enough.

<Simple2DScatterWidget>
In "Animal or Food?" — chicken, salmon, and lamb appear in the overlap because they belong to both categories. Try "Four Distinct Categories" — animals, buildings, planets, and instruments sit in clean separate quadrants. Then try "Four Overlapping Categories" — wild, water, land, and domestic animals overlap in every direction, and two dimensions can't untangle them.
</Simple2DScatterWidget>

With just two numbers, we can represent overlapping categories like a Venn diagram, or separate four distinct groups into quadrants. But we can't distinguish everything — when categories overlap in complex ways, two dimensions aren't enough. We'd need more: "is it a pet?", "does it have strings?", "is it edible?"

## Beyond Two Dimensions

Going from one dimension to two gave us dramatically more expressive power — more types of things, more ways they can differ, more ways categories can overlap. Adding more dimensions keeps making this better. With 50 or 100 dimensions, you can capture remarkably fine-grained distinctions.

This is the idea we introduced earlier: don't worry about labeling dimensions. Just put similar things close together, and let directions mean different things in different parts of the space. With enough dimensions, this works remarkably well.

Representing something as a vector like this is called an **embedding**.

<KeyInsight>
The dimensions don't need labels. What matters is that similar things are nearby and that directions encode meaningful relationships — even if the same direction means different things in different regions of the space.
</KeyInsight>

## Exploring a Real Embedding

Let's try a real embedding — GloVe, created by Stanford. Each word is a point in 50-dimensional space. The full embedding contains 400,000 words, which is too many to fit in the browser, so we're showing a subset of common nouns. We can't visualize 50 dimensions, but we *can* use cosine similarity (from the previous chapter) to ask: what's nearby?

<EmbeddingPlaygroundWidget>
Start with "dog" — its neighbors are other animals and pets. Click on a neighbor to jump to it and browse around the space. Try "guitar" — you get other instruments. Try "queen" — notice it sits near royalty words. Type your own words and see what clusters together.
</EmbeddingPlaygroundWidget>

Notice that the embedding groups words by meaning without being told what any word means. "Dog" is near "cat" and "puppy". "Guitar" is near "piano" and "bass". Similar words end up nearby — exactly the idea we described earlier.

## Directions Have Meaning

The embedding doesn't just group similar words together — *directions* in the space carry meaning too. The direction from "ant" to "whale" captures something like size. The direction from "boy" to "man" captures age. You can pick any two words, and the line between them defines a meaningful spectrum.

<WordPairSpectrumWidget>
Try "ant ↔ whale" — notice how animals arrange from small and land-dwelling to large and aquatic, the two main ways ants and whales differ. Try "salad ↔ cake" for a healthy-to-indulgent food spectrum. Try "boy ↔ man" for an age spectrum. Pick your own word pairs and see what spectrums emerge. You'll sometimes find unexpected words in between — sometimes because the embedding is imperfect, and sometimes because it's capturing dimensions of meaning other than the one you were thinking of.
</WordPairSpectrumWidget>

This is why unlabeled dimensions are so powerful. Even though no dimension is labeled "size" or "age," those concepts exist as directions in the space. And there are far more meaningful directions than there could ever be labeled dimensions — every pair of words defines a potential axis of meaning.

<KeyInsight>
In a well-trained embedding, directions carry meaning. The direction from small to large, from young to old, from simple to complex — these all exist as paths through the space. This structure emerges from training on text, with no human labeling.
</KeyInsight>


## From Words to Tokens (Subwords)

Real language models do **not** treat each full word as one unit.
Instead, they break text into **tokens**.

A token can be:
- a whole word (`cat`)
- part of a word (`-ing`)
- punctuation and other symbols (`!`)

Why break words into parts?

Because language has way too many possible words.
People invent new words, spell things differently, and use rare words all the time. If we tried to create an embedding for every possible word then the data set would get impractically large, and rare words wouldn't have been seen enough times for the embedding to know what they meant.

Take for example the word `treeishness`. You've probably never seen that word before, but you have a pretty good idea what it means because you can break it down into `tree`, `-ish`, and `-ness`.

Modern language models solve the problem the same way. The most common words get their own token, but others get assembled out of smaller parts. Try it out below.

<TokenizationPlaygroundWidget>
Click "superman, superb, superlative" and look for repeated chunks.
Then try a made-up word like "qwertyflorp" and see how it still gets broken into smaller known parts.
Try punctuation too (`!`, `,`, `?`) and notice those are tokens as well.
</TokenizationPlaygroundWidget>



## Where Do Embeddings Come From?

In modern AIs, the embedding is just the first layer of the neural network. It has one input node for every possible token. When a token comes in, that input is set to 1 and every other input is set to 0 — like raising your hand in a crowd when someone calls your name. This is called **one-hot encoding**.

Each connection carries a weight. Since only one input word is "on," the output of the first layer is just the weights for that one word. Those weights *are* the embedding. Unlike the neurons we saw in previous chapters, the embedding layer has no bias and no activation function.

The network learns those weights through gradient descent, just like any other weights - naturally assigning each word the meaning that helps the network the most.

Modern language models use much bigger embeddings than our 50-dimensional GloVe example: GPT-2 uses 768 dimensions, GPT-3 uses 12,288 dimensions.

<EmbeddingLayerDiagramWidget>
Select different tokens from the dropdown. Notice how the selected token's input lights up with a 1, and all the others show 0 — that's one-hot encoding. The bright lines show the weights connecting that token to the embedding layer, and those weights become the embedding values on the right. The embedding then feeds into more layers of neurons — the rest of the network. Try selecting "dog" then "cat" — their embedding values look more similar to each other than to "car".
</EmbeddingLayerDiagramWidget>




## Beyond Words

Embeddings are one of the most powerful ideas in modern AI. The pattern — take something discrete, map it to a point in continuous space, let training shape the geometry — applies far beyond words and tokens:

- **Image patches** in vision models
- **Audio frames** in speech recognition
- **Users and products** in recommendation systems
- **Molecules** in drug discovery

In every case, the same principle holds: similar things end up nearby, and the geometry of the space encodes meaningful relationships. An embedding is a learned translation from symbols to geometry — and the geometry of meaning emerges from the pressure to be useful.

<KeyInsight>
An embedding is a learned mapping from discrete symbols to continuous geometry. The network discovers what dimensions to use and what structure to create — driven entirely by the need to perform its task well. This idea extends far beyond words to images, audio, molecules, and more.
</KeyInsight>

## What's Next

We've seen that embeddings turn words into lists of numbers — points in a high-dimensional space where proximity means similarity and directions encode relationships. But how do neural networks actually *process* these lists of numbers? What math is happening inside each layer?

In the next chapter, we'll discover that every neural network layer is doing the same thing: **matrix multiplication** — a geometric transformation that rotates, stretches, and reshapes space. It sounds abstract, but it has a beautiful visual meaning you can watch unfold.

<TryItInPyTorch notebook="embeddings">
Build one-hot vectors, train your own word embeddings, explore real word analogies (king − man + woman = queen) with GloVe, and see how GPT-4 splits text into tokens.
</TryItInPyTorch>

# Embeddings and Vector Spaces

<Lead>
In the last chapter, we described animals with hand-picked properties like size and scariness. That worked because animals are one kind of thing. But words represent *everything* — and there's no universal set of properties for all of them. AI solves this by letting the network discover its own dimensions.
</Lead>

## From Animals to Words

In the previous chapter, we described animals as vectors — lists of numbers like (big, scary, hairy, cuddly, fast, fat). This worked beautifully: bears are big and scary, rabbits are small and cuddly, and the distance between their vectors captures how different they are.

But what if we want to represent *words* — not just animals, but everything? "Dog" and "democracy" and "purple" and "running." There's no single set of properties that makes sense for all of them. "How scary is the color blue?" isn't a meaningful question. "How hairy is democracy?" doesn't even make sense.

We need a different approach. Instead of choosing dimensions by hand, we'll let the network figure out *which numbers to use*. But first, let's see why the obvious approaches fail.

One idea: give each word its own input neuron. There's a "dog" neuron, a "cat" neuron, a "magnificent" neuron. When the input is "dog", the dog neuron fires and everything else is zero. But English has over 170,000 words — that's 170,000 input neurons, and the network has to independently learn what every single one means. It gets no hint that "dog" and "puppy" are related, or that "big" and "enormous" mean the same thing.

OK, so what if we just assign each word a single number instead? "Dog" = 1, "cat" = 2, "car" = 3, "puppy" = 4. Now we only need one input neuron. But the numbers are arbitrary — "dog" being 1 and "puppy" being 4 doesn't mean dogs are less than puppies. The network can't make sense of these numbers because they don't *mean* anything. Moving from 1 to 2 takes you from "dog" to "cat", but moving from 2 to 3 takes you from "cat" to "car" — there's no pattern to learn.

What if we could make the numbers actually *mean* something?

## Words on a Line

What if we gave each word a number that captures something about what it means? For example, we could score every word by how *big* it is. An ant gets a low number. An elephant gets a high number. A truck is up there too.

<WordNumberLineWidget>
Toggle between axes. On "Size", notice how elephant ends up next to truck — they're both big, but have nothing else in common. Switch to "Alive" — now animals separate from objects, but elephant and mouse are neighbors. One number captures ONE property but conflates everything else.
</WordNumberLineWidget>

The problem is clear: one dimension isn't enough. No matter which property you choose, words that are different in other ways end up as neighbors. Elephant is next to truck on the size axis. Mouse is next to flute on the alive axis.

## Being Clever with One Dimension

But we can be surprisingly clever with just one number. What if we use different *regions* of the number line for different categories, and order items *within* each region by a different property?

<CombinedNumberLineWidget>
Try each preset. In "Animals × Foods", animals are on the left ordered by size, while foods are on the right ordered by sweetness. One number encodes the category AND a property within it. Try the other presets — each packs two categories with different orderings onto one line.
</CombinedNumberLineWidget>

This is surprisingly expressive! One number tells you both *what category* something belongs to (which region it's in) and *something about it* (its position within that region). Different parts of the number line mean different things. But it still can't handle more than two categories well — we'd need more room on the line than we have.

## Adding a Second Dimension

What if we use *two* numbers? Now each word is a point on a 2D plane instead of a point on a line.

<Simple2DScatterWidget>
In "Size × Danger", bear and shark are big AND dangerous, while knife and gun are small but deadly. Switch to "Animal or Food?" — chicken, salmon, and lamb appear in the overlap because they belong to both categories. Try "Four Distinct Categories" — animals, buildings, planets, and instruments sit in clean separate quadrants. Then try "Four Overlapping Categories" — wild, water, land, and domestic animals overlap in every direction, and two dimensions can't untangle them.
</Simple2DScatterWidget>

Two dimensions are dramatically more expressive than one. With just two numbers, we can plot two independent properties at once, and even represent overlapping categories like a Venn diagram. But we still can't distinguish everything — dog and cat overlap, guitar and flute sit on top of each other. We'd need more dimensions: "is it a pet?", "does it have strings?", "is it edible?"

## Beyond Two Dimensions

We saw in the vectors chapter that more dimensions let you make finer distinctions. With 5 dimensions, you could separate animals from vehicles from instruments. With 50 or 100, you could capture remarkably fine-grained differences.

But here's the key problem for language: how do you decide *which* dimensions to use? With animals, we picked "big" and "scary" and it worked. With words? "How dangerous is the color blue?" doesn't make sense. There's no universal set of labeled dimensions that works for all words.

Real AI systems solve this by not using labeled dimensions at all. Instead, they learn a space where **similar words end up nearby** and **different words end up far apart**. The dimensions are just numbers — not "alive?" or "big?" — but the proximity structure captures meaning.

And it's not just proximity. The spatial *relationships* between nearby words carry meaning too. The direction from "small" to "big" tends to be consistent — move an ant in that direction and you get larger animals. The direction from "man" to "woman" captures gender, so applying it to "king" lands you near "queen." These regularities aren't designed in; they emerge from training, because a space with consistent structure is more useful than one without it.

This is like the difference between organizing books by (genre, length, year) versus letting a librarian arrange them so that related books are on nearby shelves. The librarian's system has no labels, but it works better because it captures relationships that no fixed labeling scheme could.

<KeyInsight>
Real embeddings don't use labeled dimensions. They learn positions where similar things are nearby and where directions between words encode consistent relationships — size, gender, tense, and countless others — all without any human labeling.
</KeyInsight>

## Neural Networks Meet Embeddings

In the vectors chapter, we saw that a neuron draws a decision boundary — a straight line through its input space. Now each input has a name — its position in the embedding encodes meaning. Can a neuron learn to read that meaning?

Let's find out. Below is a single neuron whose two inputs are a word's Size and Danger coordinates from the scatter plot above. Each word sits at its embedding position, and the neuron has to learn which words match a concept.

<EmbeddingClassifierWidget>
Start with "Dangerous" — the neuron needs to draw a boundary separating the dangerous things (bear, shark, knife, gun) from everything else. Hit Optimize and watch it learn. Then try "Big" — it draws a vertical boundary instead. Try "Big & Dangerous" and "Small & Dangerous" too — notice how the neuron tilts its boundary line differently each time to separate the right words.
</EmbeddingClassifierWidget>

This is only a toy example with two dimensions — but the same principle scales up. Real language models feed high-dimensional embeddings (hundreds or thousands of numbers per word) into neural networks with many layers, and those networks learn to read meaning from the geometry of the embedding space.

## Exploring a Real Embedding

Let's explore an actual word embedding — GloVe, trained by Stanford on 6 billion words of text. Each word is a point in 50-dimensional space. We can't visualize 50 dimensions, but we *can* ask: what's nearby?

<EmbeddingPlaygroundWidget>
Start with "dog" — its neighbors are other animals and pets. Try "guitar" — you get other instruments. Try "queen" — notice it sits near royalty words. Type your own words and see what clusters together.
</EmbeddingPlaygroundWidget>

Notice that the embedding groups words by meaning without being told what any word means. "Dog" is near "cat" and "puppy". "Guitar" is near "piano" and "bass". This structure emerged just from reading text — the system learned that words appearing in similar contexts should be nearby.

## Directions Have Meaning

The embedding doesn't just group similar words together — *directions* in the space carry meaning too. The direction from "ant" to "whale" captures something like size. The direction from "boy" to "man" captures age. You can pick any two words, and the line between them defines a meaningful spectrum.

<WordPairSpectrumWidget>
Try "ant ↔ whale" — notice how animals arrange from small to large between the endpoints. Try "salad ↔ cake" for a healthy-to-indulgent food spectrum. Try "boy ↔ man" for an age spectrum. Pick your own word pairs and see what spectrums emerge.
</WordPairSpectrumWidget>

This is why unlabeled dimensions are so powerful. Even though no dimension is labeled "size" or "age," those concepts exist as directions in the space. And there are far more meaningful directions than there could ever be labeled dimensions — every pair of words defines a potential axis of meaning.

<KeyInsight>
In a well-trained embedding, directions carry meaning. The direction from small to large, from young to old, from simple to complex — these all exist as paths through the space. This structure emerges from training on text, with no human labeling.
</KeyInsight>


## From Words to Tokens (Subwords)

Real language models do **not** treat each full word as one unit.
Instead, they break text into **tokens**.

A token can be:
- a whole word (`cat`)
- part of a word (`-ing`)
- punctuation and other symbols (`!`)

Why break words into parts?

Because language has way too many possible words. 
People invent new words, spell things differently, and use rare words all the time. If we tried to create an embedding for every possible word then the data set would get impractically large, and rare words wouldn't have been seen enough times for the embedding to know what they meant.

Take for example the word `treeishness`. You've probably never seen that word before, but you have a pretty good idea what it means because you can break it down into `tree`, `-ish`, and `-ness`.

Modern language models use tokenizers that solve the problem the same way. The most common words get their own token, but others get assembled out of smaller parts. Try it out below.

<TokenizationPlaygroundWidget>
Click "superman, superb, superlative" and look for repeated chunks.
Then try a made-up word like "qwertyflorp" and see how it still gets broken into smaller known parts.
Try punctuation too (`!`, `,`, `?`) and notice those are tokens as well.
</TokenizationPlaygroundWidget>



## Where Do Embeddings Come From?

We've seen that embeddings are lists of numbers that capture meaning. But how does a neural network get those numbers in the first place?

**Learning from context.** Some embeddings (like the GloVe ones we explored above) are trained by reading huge amounts of text and noticing which words keep showing up together. "Dog" and "cat" both appear near words like "pet", "fur", and "veterinarian" — so they end up close together in the embedding. The system figures out that words used in similar sentences probably mean similar things.

**Learning as part of a network.** In modern language models, the embedding is actually the very first layer of the neural network — the layer that takes in tokens and produces numbers for the rest of the network to work with. It looks like the network diagrams from the previous chapter: a layer of input nodes connected by weights to a layer of output nodes, which then feed into more layers.

But there's a trick for how the input works. The network has one input node for every possible token. When a token comes in, the network sets that token's input to 1 and every other input to 0. This is called **one-hot encoding** — it's like raising your hand in a crowd: only one hand goes up, and that tells everyone which person you are.

Remember from the previous chapter that each connection between neurons carries a **weight**. Since only one input is "on" (set to 1) and all the others are "off" (set to 0), the outputs just equal the weights coming from that one active input. Those weights *are* the embedding. During training, we learn the embedding through gradient descent, just like we learn any other weights.

Modern language models use much bigger embeddings than our 50-dimensional GloVe example: GPT-2 uses 768 dimensions, GPT-3 uses 12,288 dimensions.

<EmbeddingLayerDiagramWidget>
Select different tokens from the dropdown. Notice how the selected token's input lights up with a 1, and all the others show 0 — that's one-hot encoding. The bright lines show the weights connecting that token to the embedding layer, and those weights become the embedding values on the right. The embedding then feeds into more layers of neurons — the rest of the network. Try selecting "dog" then "cat" — their embedding values look more similar to each other than to "car".
</EmbeddingLayerDiagramWidget>




## Beyond Words

Embeddings are one of the most powerful ideas in modern AI. The pattern — take something discrete, map it to a point in continuous space, let training shape the geometry — applies far beyond words and tokens:

- **Image patches** in vision models
- **Audio frames** in speech recognition
- **Users and products** in recommendation systems
- **Molecules** in drug discovery

In every case, the same principle holds: similar things end up nearby, and the geometry of the space encodes meaningful relationships. An embedding is a learned translation from symbols to geometry — and the geometry of meaning emerges from the pressure to be useful.

<KeyInsight>
An embedding is a learned mapping from discrete symbols to continuous geometry. The network discovers what dimensions to use and what structure to create — driven entirely by the need to perform its task well. This idea extends far beyond words to images, audio, molecules, and more.
</KeyInsight>

## What's Next

We've seen that embeddings turn words into lists of numbers — points in a high-dimensional space where proximity means similarity and directions encode relationships. But how do neural networks actually *process* these lists of numbers? What math is happening inside each layer?

In the next chapter, we'll discover that every neural network layer is doing the same thing: **matrix multiplication** — a geometric transformation that rotates, stretches, and reshapes space. It sounds abstract, but it has a beautiful visual meaning you can watch unfold.

<TryItInPyTorch notebook="embeddings">
Build one-hot vectors, train your own word embeddings, explore real word analogies (king − man + woman = queen) with GloVe, and see how GPT-4's tokenizer splits text.
</TryItInPyTorch>

# Transformers

<Lead>
We now have all the pieces: embeddings that capture meaning, attention that lets words look at each other, and neural networks that transform information. The transformer wires these together into a single architecture — and when trained to predict the next word, something remarkable happens. It learns grammar, narrative, and common sense, from nothing but prediction.
</Lead>

## Assembly

Let's take inventory of what we've built:

- **Embeddings** (Chapter 4): Turn words into vectors that capture meaning — similar words have similar vectors.
- **Matrix transformations** (Chapter 5): Rotate, scale, and project vectors — every neural network layer is a geometric transformation.
- **Neural network layers** (Chapter 3): Stack transformations with activation functions to learn any pattern.
- **Attention** (Chapter 7): Let every word look at every other word to gather relevant context, using queries, keys, values, and dot products.

A transformer takes these pieces and wires them together in a specific, repeating pattern. Let's see how.

## One Transformer Block

Each transformer block does two things:

**Step 1: Attention.** Every word looks at every other word and gathers relevant information. This is the step where "it" figures out it refers to "cat," where verbs find their subjects, where adjectives attach to their nouns. After attention, each word's representation has been enriched with context from the rest of the sentence.

**Step 2: Feed-forward network.** A small neural network (just 2 layers) processes what attention gathered. Think of attention as *collecting information* and the feed-forward network as *thinking about it*. This is where the model transforms raw associations into useful knowledge.

That's one block. A real transformer stacks *many* of these blocks — GPT-3 has 96, for example. Each block refines the representation further. Early blocks might handle basic grammar ("this is a noun, this is a verb"). Middle blocks might resolve references ("it = the cat"). Later blocks might capture abstract meaning and world knowledge ("cats are animals that can be tired").

<Callout type="info" title="Shortcut connections">
There's one more detail: each block has a "shortcut connection" that adds the input directly to the output, bypassing both attention and the feed-forward network. Think of it like a highway bypass — the original information can always get through unchanged, and the block only needs to add *refinements*. This prevents information from getting degraded as it passes through dozens of layers. We'll explore this more in a later chapter.
</Callout>

## The Output

After the final block, each word has been transformed from a raw embedding into a rich representation that captures its meaning in context. But we need a *prediction* — a specific word.

The last step is a simple linear layer that converts this rich representation back into a score for every word in the vocabulary. "How likely is 'cat'? How likely is 'sat'? How likely is 'the'?" Softmax turns these scores into probabilities. The highest probability is the model's prediction.

That's the whole transformer: **embed → (attend → feed-forward) × N layers → predict**.

## A Tiny Transformer on Symbols

Let's watch data flow through an actual (tiny) transformer. This one has just 1 layer and works on colored symbols instead of words — small enough to see every computation.

<ToyTransformerWidget />

<TryIt>
Step through each stage: input → embeddings → attention → feed-forward → output. Watch how the embedding vectors flow through the network. Try different examples and see how the attention pattern changes. The model has learned that the most frequent color in the sequence predicts the next symbol's color.
</TryIt>

Even this toy transformer follows the same pattern as the real ones: embed the input, run attention to gather context, process with a feed-forward network, and produce output scores. The only difference is scale — a real transformer has bigger embeddings, more layers, more heads, and a vocabulary of words instead of colored shapes.

## From Symbols to Stories

Same architecture, trained on children's stories instead of symbol patterns. Now the input is words, the embeddings are richer, there are more layers and heads — but the computation is identical.

Watch what happens when it predicts the next word of real text:

<StoryTransformerWidget />

<TryIt>
Start with one of the preset prompts and click predictions to extend the story. Notice how the predictions make sense — after "Once upon a time there was a little," it predicts "girl," "boy," "dog." These are reasonable! The model has never been told what a story is — it learned narrative structure purely from predicting next words.
</TryIt>

Think about what the model had to learn to make these predictions:

- **Grammar**: "a little" → noun (not verb, not preposition)
- **Narrative conventions**: stories have characters with names, characters do things
- **Common sense**: little girls can be "named" something; they can "love" things
- **Coherence**: the prediction should fit with everything that came before

Nobody programmed any of this. The model discovered it all from one training signal: predict the next word.

## Generation: Prediction in a Loop

If you can predict the next word, you can *write*. The recipe is simple:

1. Start with a prompt
2. Predict the most likely next word
3. Append it to the text
4. Go back to step 2

This loop — predict, append, repeat — is called **auto-regressive generation**. It's exactly how ChatGPT, Claude, and every large language model generates text.

<StoryGeneratorWidget />

<TryIt>
Pick a prompt and hit Generate. Watch the story appear word by word — this is auto-regressive generation in action. Now try the temperature slider: at 0, the model always picks the highest-probability word (safe but boring). Crank it up and the model takes more risks — the stories get creative, then weird, then wild. This is the same temperature control that real chatbots use.
</TryIt>

**Temperature** controls randomness. At temperature 0, the model always picks the single most likely word — the result is predictable and repetitive. At high temperature, it samples more evenly from all possibilities — the result is creative but can veer into nonsense. The sweet spot is somewhere in the middle, and finding it is more art than science.

<KeyInsight>
A model trained *only* to predict the next word of children's stories has learned grammar, story structure, character consistency, and common sense. **Nobody programmed these things.** They emerged from prediction. Scale this up — bigger model, more data, more compute — and you get the AI systems people use every day.
</KeyInsight>

## The Big Picture

Let's zoom out and trace the path we've taken:

1. **Everything is numbers** (Chapter 1) — text, images, sound
2. **Optimization finds the right numbers** (Chapter 2) — gradient descent
3. **Neural networks are universal function finders** (Chapter 3) — neurons, layers, backprop
4. **Embeddings give words meaning** (Chapter 4) — vectors in space
5. **Matrix math is geometry** (Chapter 5) — transformations, dot products
6. **Next-word prediction requires understanding** (Chapter 6) — the training task
7. **Attention lets words look at each other** (Chapter 7) — queries, keys, values
8. **The transformer wires it all together** (this chapter) — and learns from prediction

That's the architecture behind the AI revolution. A stack of attention and feed-forward layers, trained to predict the next word, that learns language, reasoning, and world knowledge as a side effect.

The transformer you just explored has a few thousand parameters. GPT-4 has over a trillion. Claude has a similar scale. The architecture is the same — embed, attend, transform, predict. The difference is just scale: more parameters, more data, more compute. And remarkably, as you scale up, new capabilities keep *emerging* — arithmetic, reasoning, code generation, poetry — abilities that were never explicitly trained, just discovered through prediction.

In the next chapter, we'll see how a model trained this way can be adapted to specific tasks, and why a transformer trained on the internet knows enough to do almost anything.

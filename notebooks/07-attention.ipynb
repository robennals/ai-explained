{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Attention — Try it in PyTorch\n",
    "\n",
    "This is an **optional** hands-on companion to [Chapter 7](https://learnai.robennals.org/07-attention). You'll implement attention from scratch: queries, keys, values, softmax, multiple heads, and rotary position encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dotprod-header",
   "metadata": {},
   "source": [
    "## Dot Products Measure Similarity\n",
    "\n",
    "Attention is built on dot products — they measure how similar two vectors are. Let's refresh: two vectors pointing in the same direction have a high dot product, orthogonal vectors have zero, and opposite vectors are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dotprod-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot products measure similarity between vectors\n",
    "a = torch.tensor([1.0, 0.0])   # pointing right\n",
    "b = torch.tensor([0.8, 0.6])   # mostly right, a bit up\n",
    "c = torch.tensor([0.0, 1.0])   # pointing up (perpendicular to a)\n",
    "d = torch.tensor([-1.0, 0.0])  # pointing left (opposite of a)\n",
    "\n",
    "print(\"Dot products with a = [1, 0] (pointing right):\")\n",
    "print(f\"  a · b (similar direction):  {torch.dot(a, b).item():.2f}\")\n",
    "print(f\"  a · c (perpendicular):      {torch.dot(a, c).item():.2f}\")\n",
    "print(f\"  a · d (opposite):           {torch.dot(a, d).item():.2f}\")\n",
    "print(\"\\nAttention uses dot products to find which words are relevant to each other.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "softmax-header",
   "metadata": {},
   "source": [
    "## Softmax: Turning Scores into Percentages\n",
    "\n",
    "After computing dot products, we need to turn them into weights that add up to 1. **Softmax** does this — it exponentiates each score and normalizes. The biggest score dominates, and small scores shrink to nearly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "softmax-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.tensor([2.0, 1.0, 0.1, -1.0])\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "print(f\"Raw scores:     {scores.tolist()}\")\n",
    "print(f\"After softmax:  {[f'{w:.3f}' for w in weights.tolist()]}\")\n",
    "print(f\"Sum:            {weights.sum().item():.3f}\")\n",
    "print(\"\\nThe biggest score (2.0) gets most of the weight.\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "labels = [\"word A\", \"word B\", \"word C\", \"word D\"]\n",
    "ax1.bar(labels, scores.numpy())\n",
    "ax1.set_title(\"Raw dot-product scores\")\n",
    "ax1.set_ylabel(\"Score\")\n",
    "\n",
    "ax2.bar(labels, weights.numpy(), color=\"orange\")\n",
    "ax2.set_title(\"After softmax (probabilities)\")\n",
    "ax2.set_ylabel(\"Weight\")\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qkv-header",
   "metadata": {},
   "source": [
    "## Queries, Keys, and Values from Scratch\n",
    "\n",
    "Each word gets three representations:\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I offer?\"\n",
    "- **Value (V)**: \"What information do I give?\"\n",
    "\n",
    "These are computed by multiplying the word's embedding by three learned weight matrices. Let's build this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qkv-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny sentence with pre-defined embeddings\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"it\"]\n",
    "seq_len = len(words)\n",
    "embed_dim = 4\n",
    "\n",
    "# Pretend embeddings (in reality these come from nn.Embedding)\n",
    "torch.manual_seed(42)\n",
    "embeddings = torch.randn(seq_len, embed_dim)\n",
    "\n",
    "# Learned weight matrices that create Q, K, V\n",
    "head_dim = 4\n",
    "W_Q = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "W_K = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "W_V = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "\n",
    "# Compute Q, K, V for every word\n",
    "Q = W_Q(embeddings)  # (seq_len, head_dim)\n",
    "K = W_K(embeddings)\n",
    "V = W_V(embeddings)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Q shape: {Q.shape}  (one query vector per word)\")\n",
    "print(f\"K shape: {K.shape}  (one key vector per word)\")\n",
    "print(f\"V shape: {V.shape}  (one value vector per word)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-header",
   "metadata": {},
   "source": [
    "## Computing Attention Weights\n",
    "\n",
    "Now the attention mechanism:\n",
    "1. Dot product between each query and every key → relevance scores\n",
    "2. Scale by √(head_dim) to keep values from getting too large\n",
    "3. Softmax to turn scores into weights\n",
    "4. Weighted sum of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Dot products between all queries and all keys\n",
    "# Q @ K^T gives a (seq_len × seq_len) matrix of scores\n",
    "scores = Q @ K.T\n",
    "print(f\"Attention scores (before scaling):\\n{scores.detach().numpy().round(2)}\")\n",
    "\n",
    "# Step 2: Scale by sqrt(head_dim)\n",
    "scale = head_dim ** 0.5\n",
    "scores_scaled = scores / scale\n",
    "\n",
    "# Step 3: Softmax along the key dimension\n",
    "attn_weights = F.softmax(scores_scaled, dim=-1)\n",
    "print(f\"\\nAttention weights (after softmax):\")\n",
    "print(f\"Each row sums to 1 — it's how much each word attends to every other word.\")\n",
    "for i, word in enumerate(words):\n",
    "    weights_str = \"  \".join(f\"{words[j]}:{attn_weights[i,j].item():.2f}\" for j in range(seq_len))\n",
    "    print(f\"  {word:>3} attends to: {weights_str}\")\n",
    "\n",
    "# Step 4: Weighted sum of values\n",
    "output = attn_weights @ V\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(\"Each word now has a new representation enriched with context from relevant words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention pattern\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(attn_weights.detach().numpy(), cmap=\"Blues\", vmin=0, vmax=1)\n",
    "plt.xticks(range(seq_len), words)\n",
    "plt.yticks(range(seq_len), words)\n",
    "plt.xlabel(\"Attending to (keys)\")\n",
    "plt.ylabel(\"From (queries)\")\n",
    "plt.title(\"Attention weights\")\n",
    "plt.colorbar(label=\"Weight\")\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        plt.text(j, i, f\"{attn_weights[i,j].item():.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch-header",
   "metadata": {},
   "source": [
    "## Using PyTorch's Built-in Attention\n",
    "\n",
    "PyTorch provides `nn.MultiheadAttention` that does all of the above in one call. Let's verify our manual version matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's built-in scaled dot-product attention\n",
    "# (added in PyTorch 2.0)\n",
    "with torch.no_grad():\n",
    "    # Our manual Q, K, V — add a batch dimension\n",
    "    out_manual = F.scaled_dot_product_attention(\n",
    "        Q.unsqueeze(0), K.unsqueeze(0), V.unsqueeze(0)\n",
    "    ).squeeze(0)\n",
    "\n",
    "print(\"Our manual output (first word):\")\n",
    "print(f\"  {output[0].detach().numpy().round(4)}\")\n",
    "print(\"PyTorch's built-in output (first word):\")\n",
    "print(f\"  {out_manual[0].numpy().round(4)}\")\n",
    "print(\"\\nThey match! Our manual implementation is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multihead-header",
   "metadata": {},
   "source": [
    "## Multiple Attention Heads\n",
    "\n",
    "One attention head can only ask one kind of question. Multiple heads run in parallel, each with their own Q, K, V weights. One head might track grammar, another might track pronoun references, another might find the sentence topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multihead-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 3\n",
    "head_dim = 4\n",
    "\n",
    "# Each head has its own Q, K, V projections\n",
    "torch.manual_seed(0)\n",
    "heads = []\n",
    "for h in range(n_heads):\n",
    "    wq = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "    wk = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "    wv = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "    heads.append((wq, wk, wv))\n",
    "\n",
    "# Run each head and collect attention patterns\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(4 * n_heads, 3.5))\n",
    "\n",
    "for h, (wq, wk, wv) in enumerate(heads):\n",
    "    with torch.no_grad():\n",
    "        q = wq(embeddings)\n",
    "        k = wk(embeddings)\n",
    "        scores = q @ k.T / (head_dim ** 0.5)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    ax = axes[h]\n",
    "    ax.imshow(weights.numpy(), cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(words, fontsize=9)\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_yticklabels(words, fontsize=9)\n",
    "    ax.set_title(f\"Head {h + 1}\")\n",
    "\n",
    "plt.suptitle(\"Each head learns different attention patterns\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Different random weights → different attention patterns.\")\n",
    "print(\"After training, each head specializes in a different relationship type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orderblind-header",
   "metadata": {},
   "source": [
    "## Attention Is Order-Blind\n",
    "\n",
    "Here's a surprising fact: if we scramble the word order, the attention scores between the same pairs of words don't change. Attention only sees embeddings — it has no idea *where* words are in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orderblind-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original order\n",
    "original_order = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Scrambled order\n",
    "scrambled_order = [4, 2, 0, 3, 1]\n",
    "\n",
    "emb_original = embeddings[original_order]\n",
    "emb_scrambled = embeddings[scrambled_order]\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_orig = W_Q(emb_original)\n",
    "    k_orig = W_K(emb_original)\n",
    "    scores_orig = q_orig @ k_orig.T\n",
    "\n",
    "    q_scram = W_Q(emb_scrambled)\n",
    "    k_scram = W_K(emb_scrambled)\n",
    "    scores_scram = q_scram @ k_scram.T\n",
    "\n",
    "# Check: what's the score between \"cat\" (idx 1) and \"it\" (idx 4)?\n",
    "cat_it_original = scores_orig[1, 4].item()  # cat=pos1, it=pos4 in original\n",
    "# In scrambled: cat is at position 4, it is at position 0\n",
    "cat_it_scrambled = scores_scram[4, 0].item()\n",
    "\n",
    "print(f\"Score between 'cat' and 'it':\")\n",
    "print(f\"  Original order:  {cat_it_original:.4f}\")\n",
    "print(f\"  Scrambled order: {cat_it_scrambled:.4f}\")\n",
    "print(f\"  Identical! Attention doesn't know where words are.\")\n",
    "print(f\"\\n'The cat sat on it' and 'it sat The on cat' look the same to attention.\")\n",
    "print(\"We need position information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rope-header",
   "metadata": {},
   "source": [
    "## Rotary Position Encoding (RoPE)\n",
    "\n",
    "The fix: **rotate** each word's embedding by an angle based on its position. The dot product of two rotated vectors depends on the *difference* in their angles — the *relative distance* between the words.\n",
    "\n",
    "Two words 3 positions apart always have the same angular difference, whether at positions 1 & 4 or positions 50 & 53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rope-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x, positions, base=10.0):\n",
    "    \"\"\"Apply rotary position encoding to vectors.\n",
    "    \n",
    "    x: (seq_len, dim) — the vectors to rotate\n",
    "    positions: (seq_len,) — position of each vector\n",
    "    \"\"\"\n",
    "    dim = x.shape[-1]\n",
    "    # Each pair of dimensions gets rotated by a different frequency\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    # Angle = position × frequency\n",
    "    angles = positions.unsqueeze(-1) * freqs.unsqueeze(0)  # (seq_len, dim//2)\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    # Rotate pairs of dimensions\n",
    "    x1 = x[..., 0::2]  # even dimensions\n",
    "    x2 = x[..., 1::2]  # odd dimensions\n",
    "    rotated = torch.stack([\n",
    "        x1 * cos - x2 * sin,\n",
    "        x1 * sin + x2 * cos,\n",
    "    ], dim=-1).flatten(-2)\n",
    "    return rotated\n",
    "\n",
    "# Demonstrate: the dot product depends on RELATIVE position, not absolute\n",
    "dim = 8\n",
    "torch.manual_seed(7)\n",
    "vec_a = torch.randn(1, dim)\n",
    "vec_b = torch.randn(1, dim)\n",
    "\n",
    "print(\"Dot product of the same two vectors at different absolute positions:\")\n",
    "print(f\"{'Pos A':>6} {'Pos B':>6} {'Gap':>5} {'Dot Product':>12}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for pos_a, pos_b in [(1, 4), (10, 13), (50, 53), (100, 103)]:\n",
    "    ra = apply_rope(vec_a, torch.tensor([float(pos_a)]))\n",
    "    rb = apply_rope(vec_b, torch.tensor([float(pos_b)]))\n",
    "    dot = (ra * rb).sum().item()\n",
    "    print(f\"{pos_a:>6} {pos_b:>6} {pos_b-pos_a:>5} {dot:>12.4f}\")\n",
    "\n",
    "print(\"\\nSame gap = same dot product, regardless of absolute position!\")\n",
    "\n",
    "print(\"\\nNow change the gap:\")\n",
    "print(f\"{'Pos A':>6} {'Pos B':>6} {'Gap':>5} {'Dot Product':>12}\")\n",
    "print(\"-\" * 35)\n",
    "for gap in [1, 3, 5, 10, 20]:\n",
    "    ra = apply_rope(vec_a, torch.tensor([0.0]))\n",
    "    rb = apply_rope(vec_b, torch.tensor([float(gap)]))\n",
    "    dot = (ra * rb).sum().item()\n",
    "    print(f\"{0:>6} {gap:>6} {gap:>5} {dot:>12.4f}\")\n",
    "\n",
    "print(\"\\nDifferent gap = different dot product. The model can tell distance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rope-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: rotate a 2D vector by different positions\n",
    "angles_viz = torch.linspace(0, 2 * np.pi, 20)\n",
    "vec = torch.tensor([1.0, 0.0])  # unit vector pointing right\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Left: vectors at different positions\n",
    "positions = torch.arange(8).float()\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.9, len(positions)))\n",
    "for i, pos in enumerate(positions):\n",
    "    rotated = apply_rope(vec.unsqueeze(0), pos.unsqueeze(0)).squeeze()\n",
    "    ax1.arrow(0, 0, rotated[0].item() * 0.9, rotated[1].item() * 0.9,\n",
    "              head_width=0.05, color=colors[i], linewidth=2)\n",
    "    ax1.annotate(f\"pos {int(pos)}\", (rotated[0].item(), rotated[1].item()),\n",
    "                fontsize=8, ha='center')\n",
    "\n",
    "ax1.set_xlim(-1.3, 1.3)\n",
    "ax1.set_ylim(-1.3, 1.3)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title(\"Same vector rotated by position\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: dot product vs distance\n",
    "gaps = torch.arange(0, 20).float()\n",
    "dots = []\n",
    "for gap in gaps:\n",
    "    ra = apply_rope(vec_a, torch.tensor([0.0]))\n",
    "    rb = apply_rope(vec_b, torch.tensor([gap]))\n",
    "    dots.append((ra * rb).sum().item())\n",
    "\n",
    "ax2.plot(gaps.numpy(), dots, 'o-', markersize=4)\n",
    "ax2.set_xlabel(\"Distance between words (gap)\")\n",
    "ax2.set_ylabel(\"Dot product\")\n",
    "ax2.set_title(\"Dot product depends on relative distance\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full-header",
   "metadata": {},
   "source": [
    "## Putting It All Together: Self-Attention with RoPE\n",
    "\n",
    "Let's combine everything — Q/K/V projections, rotary positions, scaled dot-product attention, and multiple heads — into a complete self-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.W_qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        self.W_out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, dim = x.shape\n",
    "        # Project to Q, K, V\n",
    "        qkv = self.W_qkv(x)  # (seq_len, 3 * embed_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Reshape for multiple heads: (n_heads, seq_len, head_dim)\n",
    "        q = q.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        k = k.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        v = v.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        # Apply rotary position encoding\n",
    "        positions = torch.arange(seq_len).float()\n",
    "        q = apply_rope(q, positions.unsqueeze(0).expand(self.n_heads, -1).reshape(-1)).view_as(q)\n",
    "        k = apply_rope(k, positions.unsqueeze(0).expand(self.n_heads, -1).reshape(-1)).view_as(k)\n",
    "\n",
    "        # Scaled dot-product attention per head\n",
    "        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = attn @ v  # (n_heads, seq_len, head_dim)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        out = out.transpose(0, 1).contiguous().view(seq_len, -1)\n",
    "        return self.W_out(out), attn\n",
    "\n",
    "# Test it\n",
    "torch.manual_seed(42)\n",
    "attn_layer = SelfAttention(embed_dim=8, n_heads=2)\n",
    "test_input = torch.randn(5, 8)  # 5 words, 8-dim embeddings\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = attn_layer(test_input)\n",
    "\n",
    "print(f\"Input shape:  {test_input.shape}  (5 words, 8 dimensions)\")\n",
    "print(f\"Output shape: {output.shape}  (same — enriched with context)\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}  (2 heads, 5×5 attention matrix)\")\n",
    "print(\"\\nEach word now carries information from the words it attended to.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": "---\n\n*This notebook accompanies [Chapter 7: Attention](https://learnai.robennals.org/07-attention). The interactive widgets in the web version let you step through attention computations, see multiple heads, scramble word positions, and explore rotary encoding visually.*\n\n*New to PyTorch? See the [PyTorch from Scratch](https://learnai.robennals.org/appendix-pytorch) appendix for a beginner-friendly introduction.*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
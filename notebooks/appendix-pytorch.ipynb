{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# PyTorch from Scratch — Try it in PyTorch\n",
        "\n",
        "This notebook contains all the code from the [PyTorch from Scratch appendix](https://robennals.github.io/ai-explained/appendix-pytorch). Run each cell to follow along."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tensors-header",
      "metadata": {},
      "source": [
        "## Tensors\n",
        "\n",
        "A **tensor** is PyTorch's word for \"array of numbers.\" Think of it like a NumPy array that can also run on GPUs and track gradients."
      ]
    },
    {
      "cell_type": "code",
      "id": "tensors-create",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "# From a Python list\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(f\"Vector: {x}\")\n",
        "\n",
        "# A 2x3 matrix\n",
        "m = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                   [4.0, 5.0, 6.0]])\n",
        "print(f\"Matrix:\\n{m}\")\n",
        "\n",
        "# Zeros, ones, random\n",
        "z = torch.zeros(3, 4)\n",
        "o = torch.ones(2, 2)\n",
        "r = torch.randn(5, 3)  # random numbers from a normal distribution\n",
        "\n",
        "print(f\"\\nRandom 5x3 tensor:\\n{r}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "shape-header",
      "metadata": {},
      "source": [
        "### Shape and Dtype\n",
        "\n",
        "Every tensor has a **shape** (its dimensions) and a **dtype** (the type of number it stores). When something goes wrong, `print(x.shape)` is usually your first debugging move."
      ]
    },
    {
      "cell_type": "code",
      "id": "shape-code",
      "metadata": {},
      "source": [
        "x = torch.randn(3, 4)\n",
        "print(f\"Shape: {x.shape}\")\n",
        "print(f\"Dtype: {x.dtype}\")\n",
        "print(f\"Tensor:\\n{x}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "math-header",
      "metadata": {},
      "source": [
        "### Basic Math\n",
        "\n",
        "Tensor math works element-wise by default. For matrix multiplication (the workhorse of neural networks), use `@`."
      ]
    },
    {
      "cell_type": "code",
      "id": "math-code",
      "metadata": {},
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "print(f\"a + b = {a + b}\")\n",
        "print(f\"a * b = {a * b}\")\n",
        "print(f\"a ** 2 = {a ** 2}\")\n",
        "\n",
        "# Matrix multiplication: (2x3) @ (3x4) -> (2x4)\n",
        "A = torch.randn(2, 3)\n",
        "B = torch.randn(3, 4)\n",
        "C = A @ B\n",
        "print(f\"\\nMatrix multiply: {A.shape} @ {B.shape} -> {C.shape}\")\n",
        "print(f\"Result:\\n{C}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "autograd-header",
      "metadata": {},
      "source": [
        "## Automatic Differentiation\n",
        "\n",
        "PyTorch's killer feature. Set `requires_grad=True` on a tensor, do math with it, then call `.backward()` — PyTorch computes the gradient automatically."
      ]
    },
    {
      "cell_type": "code",
      "id": "autograd-code",
      "metadata": {},
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# y = x^2 + 2x + 1\n",
        "y = x ** 2 + 2 * x + 1\n",
        "\n",
        "# Compute gradients\n",
        "y.backward()\n",
        "\n",
        "# dy/dx = 2x + 2 = 2(3) + 2 = 8\n",
        "print(f\"x = {x.item()}\")\n",
        "print(f\"y = x^2 + 2x + 1 = {y.item()}\")\n",
        "print(f\"dy/dx = 2x + 2 = {x.grad.item()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "nn-header",
      "metadata": {},
      "source": [
        "## Building a Neural Network\n",
        "\n",
        "PyTorch provides `torch.nn` — a library of common neural network building blocks."
      ]
    },
    {
      "cell_type": "code",
      "id": "nn-linear",
      "metadata": {},
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# nn.Linear: the most fundamental layer\n",
        "# It computes y = x @ W.T + b\n",
        "layer = nn.Linear(in_features=3, out_features=2)\n",
        "x = torch.randn(1, 3)      # one input with 3 features\n",
        "y = layer(x)                # output has 2 features\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {layer.weight.shape}\")\n",
        "print(f\"Bias shape:   {layer.bias.shape}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "nn-sequential",
      "metadata": {},
      "source": [
        "# nn.Sequential: stack layers into a network\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 16),   # 2 inputs -> 16 hidden neurons\n",
        "    nn.ReLU(),           # activation function\n",
        "    nn.Linear(16, 16),  # 16 -> 16\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 1),   # 16 -> 1 output\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "xor-header",
      "metadata": {},
      "source": [
        "## Putting It Together: Learning XOR\n",
        "\n",
        "XOR returns 1 when its inputs differ, and 0 when they're the same. A single neuron can't solve it, but a two-layer network can.\n",
        "\n",
        "| Input A | Input B | Output |\n",
        "|---------|---------|--------|\n",
        "| 0       | 0       | 0      |\n",
        "| 0       | 1       | 1      |\n",
        "| 1       | 0       | 1      |\n",
        "| 1       | 1       | 0      |"
      ]
    },
    {
      "cell_type": "code",
      "id": "xor-code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training data\n",
        "inputs = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "targets = torch.tensor([[0.], [1.], [1.], [0.]])\n",
        "\n",
        "# Model: 2 inputs -> 8 hidden neurons -> 1 output\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(8, 1),\n",
        "    nn.Sigmoid(),       # squash output to 0-1\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss()  # binary cross-entropy for 0/1 classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train and track loss\n",
        "losses = []\n",
        "for epoch in range(2000):\n",
        "    predictions = model(inputs)\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n",
        "\n",
        "# Plot loss over time\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training loss — the network learns XOR\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    results = model(inputs)\n",
        "    print(\"\\nResults:\")\n",
        "    for inp, target, out in zip(inputs, targets, results):\n",
        "        print(f\"  {inp.tolist()} -> {out.item():.3f}  (target: {target.item():.0f})\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "footer",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "*This notebook accompanies [Appendix: PyTorch from Scratch](https://robennals.github.io/ai-explained/appendix-pytorch). Now that you know the basics, try the \"Try it in PyTorch\" notebooks in each chapter!*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

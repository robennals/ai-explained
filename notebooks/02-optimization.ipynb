{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# The Power of Incremental Improvement — Try it in PyTorch\n",
    "\n",
    "This is an **optional** hands-on companion to [Chapter 2](https://robennals.github.io/ai-explained/02-optimization). You'll implement random search, compute gradients with PyTorch's autograd, and watch gradient descent converge."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "random-header",
   "metadata": {},
   "source": "## Random Search\n\nOur goal is to find the input value that makes a function output the smallest possible number. We call that output the **loss** — it measures \"how wrong\" we are. A loss of 0 means perfect; bigger means worse. The process of finding the input that minimizes the loss is called **optimization**."
  },
  {
   "cell_type": "code",
   "id": "random-code",
   "metadata": {},
   "source": [
    "# Goal: find the x that minimizes f(x) = (x - 3)² + 1\n",
    "# Strategy: start somewhere, try random small changes, keep improvements\n",
    "\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 1\n",
    "\n",
    "x = torch.tensor(0.0)\n",
    "best_loss = f(x)\n",
    "history = [x.item()]\n",
    "\n",
    "for step in range(200):\n",
    "    delta = torch.randn(1).item() * 0.5\n",
    "    x_new = x + delta\n",
    "    new_loss = f(x_new)\n",
    "    \n",
    "    if new_loss < best_loss:\n",
    "        x = x_new\n",
    "        best_loss = new_loss\n",
    "    history.append(x.item())\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(history)\n",
    "plt.axhline(y=3.0, color='r', linestyle='--', alpha=0.5, label='True minimum (x=3)')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"x value\")\n",
    "plt.title(\"Random search: stumbling toward the answer\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Found x = {x.item():.4f}, f(x) = {f(x).item():.4f}  (true minimum: x=3, f(x)=1)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "grad-header",
   "metadata": {},
   "source": "## Smoothness and Gradients\n\nA **gradient** is just the slope of a function — it tells you which direction is \"downhill.\" If you're standing on a hillside, the gradient points toward the steepest uphill direction. To minimize the loss, you walk the opposite way.\n\n`requires_grad=True` tells PyTorch: \"keep track of every math operation on this number, so you can compute the slope later.\" Calling `.backward()` then calculates the gradient automatically. How this works internally (backpropagation) is covered in Chapter 3 — for now, just think of it as PyTorch doing the calculus for you."
  },
  {
   "cell_type": "code",
   "id": "grad-code",
   "metadata": {},
   "source": [
    "# PyTorch can automatically compute gradients (slopes)!\n",
    "# This is called \"autograd\" — automatic differentiation\n",
    "\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Compute the function\n",
    "loss = (x - 3)**2 + 1\n",
    "\n",
    "# Compute the gradient (how does loss change when x changes?)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"x = {x.item():.1f}\")\n",
    "print(f\"f(x) = {loss.item():.1f}\")\n",
    "print(f\"gradient df/dx = {x.grad.item():.1f}\")\n",
    "print(f\"\\nThe gradient is {x.grad.item():.1f}, meaning:\")\n",
    "print(f\"  - The function is {'decreasing' if x.grad.item() < 0 else 'increasing'} at x={x.item()}\")\n",
    "print(f\"  - To decrease f(x), move x in the {'positive' if x.grad.item() < 0 else 'negative'} direction\")\n",
    "\n",
    "# Note: autograd uses backpropagation internally — we'll cover that in Chapter 3"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gd-header",
   "metadata": {},
   "source": "## Gradient Descent\n\n**Gradient descent** repeats a simple loop: compute the gradient, take a small step downhill, repeat. The **learning rate** controls how big each step is — too big and you overshoot, too small and it takes forever."
  },
  {
   "cell_type": "code",
   "id": "gd-code",
   "metadata": {},
   "source": [
    "# Gradient descent: follow the slope downhill, step by step\n",
    "\n",
    "learning_rate = 0.1\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "history_x = [x.item()]\n",
    "history_loss = []\n",
    "\n",
    "for step in range(50):\n",
    "    loss = (x - 3)**2 + 1\n",
    "    history_loss.append(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    x.grad.zero_()\n",
    "    history_x.append(x.item())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "ax1.plot(history_x)\n",
    "ax1.axhline(y=3.0, color='r', linestyle='--', alpha=0.5, label='Target (x=3)')\n",
    "ax1.set_xlabel(\"Step\")\n",
    "ax1.set_ylabel(\"x value\")\n",
    "ax1.set_title(\"x converges to the minimum\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history_loss)\n",
    "ax2.set_xlabel(\"Step\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Loss decreases smoothly\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Final: x = {x.item():.6f}, loss = {((x.item()-3)**2+1):.6f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## Random Search vs Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "id": "compare-code",
   "metadata": {},
   "source": [
    "# Compare random search vs gradient descent on a 2D problem\n",
    "# f(x, y) = (x - 2)² + (y + 1)² + 3\n",
    "\n",
    "def f_2d(x, y):\n",
    "    return (x - 2)**2 + (y + 1)**2 + 3\n",
    "\n",
    "# --- Random search ---\n",
    "torch.manual_seed(42)\n",
    "rx, ry = torch.tensor(0.0), torch.tensor(0.0)\n",
    "best_loss = f_2d(rx, ry)\n",
    "random_path = [(rx.item(), ry.item())]\n",
    "\n",
    "for _ in range(200):\n",
    "    dx, dy = torch.randn(2) * 0.3\n",
    "    nx, ny = rx + dx, ry + dy\n",
    "    nl = f_2d(nx, ny)\n",
    "    if nl < best_loss:\n",
    "        rx, ry, best_loss = nx, ny, nl\n",
    "    random_path.append((rx.item(), ry.item()))\n",
    "\n",
    "# --- Gradient descent ---\n",
    "gx = torch.tensor(0.0, requires_grad=True)\n",
    "gy = torch.tensor(0.0, requires_grad=True)\n",
    "grad_path = [(gx.item(), gy.item())]\n",
    "lr = 0.1\n",
    "\n",
    "for _ in range(50):\n",
    "    loss = f_2d(gx, gy)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        gx -= lr * gx.grad\n",
    "        gy -= lr * gy.grad\n",
    "    gx.grad.zero_()\n",
    "    gy.grad.zero_()\n",
    "    grad_path.append((gx.item(), gy.item()))\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for ax, path, title, color in [\n",
    "    (ax1, random_path, \"Random search (200 steps)\", \"blue\"),\n",
    "    (ax2, grad_path, \"Gradient descent (50 steps)\", \"red\"),\n",
    "]:\n",
    "    xs, ys = zip(*path)\n",
    "    ax.plot(xs, ys, '-o', markersize=2, color=color, alpha=0.6)\n",
    "    ax.plot(xs[0], ys[0], 'ko', markersize=8, label='Start')\n",
    "    ax.plot(2, -1, 'r*', markersize=15, label='True minimum')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-3, 2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rp = random_path[-1]\n",
    "print(f\"Random search: ({rp[0]:.3f}, {rp[1]:.3f}), loss = {f_2d(torch.tensor(rp[0]), torch.tensor(rp[1])):.4f}\")\n",
    "print(f\"Gradient descent: ({gx.item():.3f}, {gy.item():.3f}), loss = {f_2d(gx, gy).item():.4f}\")\n",
    "print(f\"\\nGradient descent found the minimum in 50 steps. Random search used 200 and was less precise.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook accompanies [Chapter 2: The Power of Incremental Improvement](https://robennals.github.io/ai-explained/02-optimization). The interactive widgets in the web version let you explore these concepts visually.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Matrix Math and Linear Transformations — Try it in PyTorch\n",
    "\n",
    "This is an **optional** hands-on companion to [Chapter 5](https://learnai.robennals.org/05-matrix-math). You'll build matrices, watch them transform shapes, and see that a neural network layer is really just a matrix operation."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "basics-header",
   "metadata": {},
   "source": [
    "## Scalars, Vectors, and Matrices\n",
    "\n",
    "These are just names for tensors of different sizes:\n",
    "- A **scalar** is a single number (0 dimensions)\n",
    "- A **vector** is a list of numbers (1 dimension)\n",
    "- A **matrix** is a grid of numbers (2 dimensions — rows and columns)\n",
    "\n",
    "You've been using all of these already! Now we'll see what happens when you *multiply* matrices together."
   ]
  },
  {
   "cell_type": "code",
   "id": "basics-code",
   "metadata": {},
   "source": [
    "# Scalar — a single number\n",
    "scalar = torch.tensor(3.14)\n",
    "print(f\"Scalar: {scalar}  (shape: {scalar.shape})\")\n",
    "\n",
    "# Vector — a list of numbers\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Vector: {vector}  (shape: {vector.shape})\")\n",
    "\n",
    "# Matrix — a grid of numbers (rows × columns)\n",
    "matrix = torch.tensor([[1.0, 2.0],\n",
    "                        [3.0, 4.0],\n",
    "                        [5.0, 6.0]])\n",
    "print(f\"Matrix:\\n{matrix}  (shape: {matrix.shape} = 3 rows × 2 columns)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "transform-header",
   "metadata": {},
   "source": [
    "## 2D Transformations\n",
    "\n",
    "Multiplying a 2D point by a 2×2 matrix moves that point to a new location. Apply the same matrix to every point of a shape and you get a transformed shape. Different matrices produce different transformations: rotation, stretching, shearing, and more.\n",
    "\n",
    "The `@` symbol in PyTorch means **matrix multiplication**."
   ]
  },
  {
   "cell_type": "code",
   "id": "transform-code",
   "metadata": {},
   "source": [
    "# Define a triangle as 3 points (each point is a 2D vector)\n",
    "triangle = torch.tensor([[0.0, 0.0], [1.0, 0.0], [0.5, 1.0]])\n",
    "\n",
    "# Different 2×2 matrices = different transformations\n",
    "angle = torch.tensor(torch.pi / 4)  # 45 degrees\n",
    "transforms = {\n",
    "    \"Original\": torch.eye(2),\n",
    "    \"Rotate 45°\": torch.tensor([[torch.cos(angle), -torch.sin(angle)],\n",
    "                                  [torch.sin(angle),  torch.cos(angle)]]),\n",
    "    \"Stretch X\": torch.tensor([[2.0, 0.0],\n",
    "                                [0.0, 1.0]]),\n",
    "    \"Shear\": torch.tensor([[1.0, 0.5],\n",
    "                            [0.0, 1.0]]),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "for ax, (name, M) in zip(axes, transforms.items()):\n",
    "    # Apply the transformation: multiply each point by the matrix\n",
    "    transformed = triangle @ M.T  # @ means matrix multiply\n",
    "    \n",
    "    # Close the triangle for plotting\n",
    "    pts = torch.cat([transformed, transformed[:1]])\n",
    "    ax.fill(pts[:, 0].numpy(), pts[:, 1].numpy(), alpha=0.3)\n",
    "    ax.plot(pts[:, 0].numpy(), pts[:, 1].numpy(), 'o-', markersize=6)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlim(-1.5, 2.5)\n",
    "    ax.set_ylim(-1, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dims-header",
   "metadata": {},
   "source": [
    "## Changing Dimensions\n",
    "\n",
    "Matrices don't have to be square. A matrix with different numbers of rows and columns can change the number of dimensions in your data:\n",
    "- A 1×2 matrix turns a 2D point into a single number (**projection** — losing information)\n",
    "- A 3×2 matrix turns a 2D point into a 3D point (**embedding** — adding room for more information)"
   ]
  },
  {
   "cell_type": "code",
   "id": "dims-code",
   "metadata": {},
   "source": [
    "point_2d = torch.tensor([3.0, 4.0])\n",
    "\n",
    "# Project from 2D down to 1D\n",
    "project = torch.tensor([[0.6, 0.8]])  # 1×2 matrix\n",
    "result_1d = project @ point_2d\n",
    "print(f\"2D point: {point_2d.tolist()}\")\n",
    "print(f\"After 1×2 projection: {result_1d.tolist()}  (2D → 1D)\")\n",
    "\n",
    "# Embed from 2D up to 3D\n",
    "embed = torch.tensor([[1.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [0.5, 0.5]])  # 3×2 matrix\n",
    "result_3d = embed @ point_2d\n",
    "print(f\"After 3×2 embedding: {result_3d.tolist()}  (2D → 3D)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nn-header",
   "metadata": {},
   "source": [
    "## Neural Network Layer = Matrix Operation\n",
    "\n",
    "Here's the key insight: `nn.Linear` (which we used in Chapters 3 and 4) is *just* a matrix multiplication plus a bias. Let's verify this by doing the math by hand and comparing."
   ]
  },
  {
   "cell_type": "code",
   "id": "nn-code",
   "metadata": {},
   "source": [
    "# Create a layer with 3 inputs and 2 outputs\n",
    "torch.manual_seed(42)\n",
    "layer = nn.Linear(3, 2)\n",
    "\n",
    "# Peek inside: it's just a weight matrix and a bias vector\n",
    "W = layer.weight.data  # 2×3 matrix\n",
    "b = layer.bias.data    # 2-element vector\n",
    "print(f\"Weight matrix (2×3):\\n{W}\")\n",
    "print(f\"Bias vector: {b}\")\n",
    "\n",
    "# Pass an input through the layer\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "layer_output = layer(x)\n",
    "\n",
    "# Do the same thing by hand: matrix multiply + bias\n",
    "manual_output = W @ x + b\n",
    "\n",
    "print(f\"\\nInput: {x.tolist()}\")\n",
    "print(f\"nn.Linear output:  {layer_output.tolist()}\")\n",
    "print(f\"Manual W@x + b:    {manual_output.tolist()}\")\n",
    "print(f\"\\nThey're identical! nn.Linear is just matrix multiplication + bias.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "activation-header",
   "metadata": {},
   "source": [
    "## ReLU vs Sigmoid\n",
    "\n",
    "An **activation function** is applied after each matrix multiplication. You've already seen sigmoid (squashes to 0–1). **ReLU** is even simpler: it just replaces negative numbers with zero. Despite being so simple, ReLU works great in practice and trains faster than sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "id": "activation-code",
   "metadata": {},
   "source": [
    "x = torch.linspace(-4, 4, 200)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "ax1.plot(x.numpy(), torch.sigmoid(x).numpy(), linewidth=2)\n",
    "ax1.set_title(\"Sigmoid: squashes to (0, 1)\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel(\"Input\")\n",
    "ax1.set_ylabel(\"Output\")\n",
    "\n",
    "ax2.plot(x.numpy(), torch.relu(x).numpy(), linewidth=2, color='orange')\n",
    "ax2.set_title(\"ReLU: zero if negative, unchanged if positive\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel(\"Input\")\n",
    "ax2.set_ylabel(\"Output\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "why-header",
   "metadata": {},
   "source": [
    "## Why Activation Functions Matter\n",
    "\n",
    "Without an activation function between layers, stacking two matrix multiplications is the same as a single matrix multiplication — depth is an illusion! Adding a nonlinear activation (like ReLU) between layers makes each layer do something genuinely new."
   ]
  },
  {
   "cell_type": "code",
   "id": "why-code",
   "metadata": {},
   "source": [
    "# Two matrices without activation = one matrix\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = torch.tensor([[0.5, -1.0], [1.5, 0.5]])\n",
    "\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "\n",
    "# Apply A then B (no activation)\n",
    "without_activation = B @ (A @ x)\n",
    "\n",
    "# This is the same as applying the combined matrix BA\n",
    "combined = B @ A\n",
    "combined_result = combined @ x\n",
    "\n",
    "print(\"Without activation function:\")\n",
    "print(f\"  B × (A × x) = {without_activation.tolist()}\")\n",
    "print(f\"  (BA) × x    = {combined_result.tolist()}\")\n",
    "print(f\"  Same result! Two layers collapsed into one.\")\n",
    "\n",
    "# Now with ReLU between layers\n",
    "with_activation = B @ torch.relu(A @ x)\n",
    "\n",
    "print(f\"\\nWith ReLU activation between layers:\")\n",
    "print(f\"  B × relu(A × x) = {with_activation.tolist()}\")\n",
    "print(f\"  Different! The activation makes the second layer genuinely useful.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": "---\n\n*This notebook accompanies [Chapter 5: Matrix Math and Linear Transformations](https://learnai.robennals.org/05-matrix-math). The interactive widgets in the web version let you explore these concepts visually.*\n\n*New to PyTorch? See the [PyTorch from Scratch](https://learnai.robennals.org/appendix-pytorch) appendix for a beginner-friendly introduction.*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Embeddings and Vector Spaces — Try it in PyTorch\n",
    "\n",
    "This is an **optional** hands-on companion to [Chapter 4](https://learnai.robennals.org/04-embeddings). You'll build one-hot vectors, train your own embeddings, explore real word analogies with GloVe, and see how modern tokenizers work."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "onehot-header",
   "metadata": {},
   "source": "## One-Hot Encoding\n\nBefore a neural network can work with words, we need to turn them into numbers. The simplest approach is **one-hot encoding**: give each word a vector (a list of numbers) that's all zeros except for a single 1 in a unique position. It works, but it has a big problem..."
  },
  {
   "cell_type": "code",
   "id": "onehot-code",
   "metadata": {},
   "source": [
    "# One-hot encoding: each word gets a vector with a single 1\n",
    "words = [\"cat\", \"dog\", \"fish\", \"car\"]\n",
    "one_hot = torch.eye(len(words))\n",
    "\n",
    "print(\"One-hot encodings:\")\n",
    "for word, vec in zip(words, one_hot):\n",
    "    print(f\"  {word:>4}: {vec.tolist()}\")\n",
    "\n",
    "# Problem: all words are equally distant from each other!\n",
    "print(\"\\nDistances between words (Euclidean):\")\n",
    "for i in range(len(words)):\n",
    "    for j in range(i+1, len(words)):\n",
    "        dist = torch.dist(one_hot[i], one_hot[j])\n",
    "        print(f\"  {words[i]:>4} ↔ {words[j]:<4}: {dist.item():.3f}\")\n",
    "\n",
    "print(\"\\nEvery pair has distance √2 ≈ 1.414 — 'cat' is as far from 'dog' as from 'car'!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "learn-header",
   "metadata": {},
   "source": "## Learning Embeddings\n\nAn **embedding** is a short list of numbers that represents a word (or any item). Unlike one-hot vectors, embeddings are *learned* — the network adjusts the numbers during training so that similar words end up close together.\n\n`nn.Embedding(6, 2)` creates an embedding table: 6 words, each represented by 2 numbers. We use just 2 dimensions here so we can plot them on a flat graph. Real embeddings use hundreds of dimensions."
  },
  {
   "cell_type": "code",
   "id": "learn-code",
   "metadata": {},
   "source": [
    "# Instead of one-hot, learn a dense vector for each word\n",
    "# Words that appear in similar contexts should end up close together\n",
    "\n",
    "vocab = [\"cat\", \"dog\", \"kitten\", \"puppy\", \"car\", \"truck\"]\n",
    "similar_pairs = [(0,2), (1,3), (4,5), (0,1), (2,3)]  # cat-kitten, dog-puppy, etc.\n",
    "\n",
    "# Create a 2D embedding (so we can visualize it)\n",
    "torch.manual_seed(42)\n",
    "embedding = nn.Embedding(len(vocab), 2)\n",
    "optimizer = torch.optim.Adam(embedding.parameters(), lr=0.05)\n",
    "\n",
    "# Train: make similar words close, random words far\n",
    "for epoch in range(500):\n",
    "    total_loss = 0\n",
    "    for i, j in similar_pairs:\n",
    "        vi = embedding(torch.tensor(i))\n",
    "        vj = embedding(torch.tensor(j))\n",
    "        # Pull similar words together\n",
    "        loss = torch.dist(vi, vj)**2\n",
    "        \n",
    "        # Push a random word away\n",
    "        k = torch.randint(len(vocab), (1,)).item()\n",
    "        vk = embedding(torch.tensor(k))\n",
    "        loss -= 0.3 * torch.dist(vi, vk)**2\n",
    "        loss += 2.0  # Keep loss positive\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "# Visualize the learned embeddings\n",
    "plt.figure(figsize=(6, 5))\n",
    "with torch.no_grad():\n",
    "    for i, word in enumerate(vocab):\n",
    "        vec = embedding(torch.tensor(i))\n",
    "        plt.plot(vec[0], vec[1], 'o', markersize=10)\n",
    "        plt.annotate(word, (vec[0]+0.02, vec[1]+0.02), fontsize=12)\n",
    "\n",
    "plt.title(\"Learned 2D embeddings: similar words cluster together\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "analogy-header",
   "metadata": {},
   "source": "## Word Analogies with Real Embeddings\n\nThis section downloads pre-trained **GloVe** embeddings (~66MB) from Stanford. These are real word vectors trained on billions of words of text. We use **cosine similarity** to compare vectors — it measures whether two vectors point in the same direction, regardless of their length (1.0 = identical direction, 0 = unrelated, -1 = opposite)."
  },
  {
   "cell_type": "code",
   "id": "analogy-code",
   "metadata": {},
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "glove_path = \"glove.6B.50d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    print(\"Downloading GloVe embeddings (~66MB)...\")\n",
    "    url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    urllib.request.urlretrieve(url, \"glove.6B.zip\")\n",
    "    with zipfile.ZipFile(\"glove.6B.zip\", 'r') as z:\n",
    "        z.extract(\"glove.6B.50d.txt\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "# Load embeddings\n",
    "print(\"Loading embeddings...\")\n",
    "word2vec = {}\n",
    "with open(glove_path, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vec = torch.tensor([float(x) for x in parts[1:]])\n",
    "        word2vec[word] = vec\n",
    "print(f\"Loaded {len(word2vec)} word vectors\")\n",
    "\n",
    "def analogy(a, b, c):\n",
    "    \"\"\"a is to b as c is to ???\"\"\"\n",
    "    target = word2vec[b] - word2vec[a] + word2vec[c]\n",
    "    best_word, best_sim = None, -1\n",
    "    for word, vec in word2vec.items():\n",
    "        if word in (a, b, c):\n",
    "            continue\n",
    "        sim = torch.cosine_similarity(target.unsqueeze(0), vec.unsqueeze(0))\n",
    "        if sim > best_sim:\n",
    "            best_word, best_sim = word, sim.item()\n",
    "    return best_word\n",
    "\n",
    "print(f\"\\nking - man + woman = {analogy('man', 'king', 'woman')}\")\n",
    "print(f\"paris - france + japan = {analogy('france', 'paris', 'japan')}\")\n",
    "print(f\"slow - slower + fast = {analogy('slow', 'slower', 'fast')}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "token-header",
   "metadata": {},
   "source": "## Tokenization\n\nModern AI models don't work with whole words — they split text into **tokens**, which are pieces of words. Common words like \"the\" stay whole, but rare words get broken into smaller chunks. This lets the model handle any word, even ones it's never seen before, by combining familiar pieces.\n\n`tiktoken` is the tokenizer used by GPT-4. You'll need to install it: `pip install tiktoken`"
  },
  {
   "cell_type": "code",
   "id": "token-code",
   "metadata": {},
   "source": [
    "# Modern AI models don't work with whole words — they use tokens\n",
    "# Tokens are pieces of words (subword units)\n",
    "\n",
    "# pip install tiktoken\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "examples = [\n",
    "    \"Hello, world!\",\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Supercalifragilisticexpialidocious\",\n",
    "    \"PyTorch is awesome!\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    tokens = enc.encode(text)\n",
    "    decoded = [enc.decode([t]) for t in tokens]\n",
    "    print(f\"Text: {text!r}\")\n",
    "    print(f\"  Token IDs: {tokens}\")\n",
    "    print(f\"  Tokens:    {decoded}\")\n",
    "    print(f\"  Count:     {len(tokens)} tokens\")\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": "---\n\n*This notebook accompanies [Chapter 4: Embeddings and Vector Spaces](https://learnai.robennals.org/04-embeddings). The interactive widgets in the web version let you explore these concepts visually.*\n\n*New to PyTorch? See the [PyTorch from Scratch](https://learnai.robennals.org/appendix-pytorch) appendix for a beginner-friendly introduction.*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
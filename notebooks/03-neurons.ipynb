{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Neural Networks — Try it in PyTorch\n",
    "\n",
    "This is an **optional** hands-on companion to [Chapter 3](https://robennals.github.io/ai-explained/03-neurons). You'll build neurons from scratch, see why a single neuron can't solve XOR, and watch a two-layer network learn a nonlinear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "neuron-header",
   "metadata": {},
   "source": "## A Single Neuron\n\nAn artificial **neuron** takes some numbers as input, multiplies each one by a **weight** (how important that input is), adds them up, adds a **bias** (a constant offset), and passes the result through an **activation function** that squashes the output into a useful range. That's it — the whole thing is just multiply, add, squash."
  },
  {
   "cell_type": "code",
   "id": "neuron-code",
   "metadata": {},
   "source": [
    "# A neuron: weighted sum of inputs + bias, passed through an activation function\n",
    "# output = sigmoid(w1*x1 + w2*x2 + b)\n",
    "\n",
    "inputs = torch.tensor([0.8, 0.2])     # Two inputs\n",
    "weights = torch.tensor([0.5, -0.3])   # How much each input matters\n",
    "bias = torch.tensor(0.1)               # Shifts the threshold\n",
    "\n",
    "# Step 1: Weighted sum + bias\n",
    "z = torch.dot(weights, inputs) + bias\n",
    "print(f\"Inputs:  {inputs.tolist()}\")\n",
    "print(f\"Weights: {weights.tolist()}\")\n",
    "print(f\"Bias:    {bias.item()}\")\n",
    "print(f\"Weighted sum: {weights[0]}×{inputs[0]} + {weights[1]}×{inputs[1]} + {bias.item()} = {z.item():.3f}\")\n",
    "\n",
    "# Step 2: Activation function (sigmoid squashes to 0-1)\n",
    "output = torch.sigmoid(z)\n",
    "print(f\"After sigmoid: {output.item():.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sigmoid-header",
   "metadata": {},
   "source": [
    "## The Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "sigmoid-code",
   "metadata": {},
   "source": [
    "# Sigmoid squashes any number into the range (0, 1)\n",
    "x = torch.linspace(-8, 8, 200)\n",
    "y = torch.sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(x.numpy(), y.numpy(), linewidth=2)\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "for val in [-4, -2, 0, 2, 4]:\n",
    "    sv = torch.sigmoid(torch.tensor(float(val)))\n",
    "    plt.plot(val, sv.item(), 'ro', markersize=6)\n",
    "    plt.annotate(f'σ({val})={sv.item():.2f}', (val, sv.item()),\n",
    "                textcoords=\"offset points\", xytext=(10, 10), fontsize=8)\n",
    "\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Sigmoid: squashes everything to (0, 1)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "logic-header",
   "metadata": {},
   "source": [
    "## Logic Gates with Neurons"
   ]
  },
  {
   "cell_type": "code",
   "id": "logic-code",
   "metadata": {},
   "source": [
    "# A single neuron can implement logic gates by choosing the right weights\n",
    "\n",
    "def neuron(x1, x2, w1, w2, b):\n",
    "    \"\"\"Single neuron with sigmoid activation.\"\"\"\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    return torch.sigmoid(torch.tensor(float(z)))\n",
    "\n",
    "# AND gate: both inputs must be high\n",
    "print(\"AND gate (w1=10, w2=10, b=-15):\")\n",
    "for x1, x2 in [(0,0), (0,1), (1,0), (1,1)]:\n",
    "    out = neuron(x1, x2, 10, 10, -15)\n",
    "    print(f\"  ({x1}, {x2}) → {out.item():.4f}  ≈ {round(out.item())}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# OR gate: at least one input must be high\n",
    "print(\"OR gate (w1=10, w2=10, b=-5):\")\n",
    "for x1, x2 in [(0,0), (0,1), (1,0), (1,1)]:\n",
    "    out = neuron(x1, x2, 10, 10, -5)\n",
    "    print(f\"  ({x1}, {x2}) → {out.item():.4f}  ≈ {round(out.item())}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xor-header",
   "metadata": {},
   "source": "## XOR: A Single Neuron Fails\n\nNow let's try to **train** a neuron — let the computer find the right weights automatically using gradient descent (from Chapter 2).\n\nSome new PyTorch tools:\n- `nn.Linear(2, 1)` — a neuron with 2 inputs and 1 output. It handles the weights and bias for you. (Internally it uses matrix multiplication — see Chapter 5.)\n- `nn.Sequential(...)` — chains layers together: the output of one feeds into the next.\n- `nn.BCELoss()` — a loss function for yes/no problems. It measures how far the prediction is from the correct answer.\n- `torch.optim.SGD(...)` — an **optimizer** that does the \"take a step downhill\" part of gradient descent automatically."
  },
  {
   "cell_type": "code",
   "id": "xor-code",
   "metadata": {},
   "source": [
    "# XOR: output is 1 when inputs differ, 0 when they're the same\n",
    "# A single neuron CANNOT learn this!\n",
    "\n",
    "X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0],[1],[1],[0]], dtype=torch.float32)\n",
    "\n",
    "# Single neuron (1 layer, no hidden units)\n",
    "model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Single neuron on XOR: loss gets stuck!\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Single neuron predictions on XOR:\")\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        pred = model(X[i]).item()\n",
    "        print(f\"  ({X[i,0].int().item()}, {X[i,1].int().item()}) → {pred:.3f}  (target: {y[i].item():.0f})\")\n",
    "print(\"\\nThe single neuron can't separate XOR — it needs a hidden layer!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "twolayer-header",
   "metadata": {},
   "source": [
    "## Two Layers Solve XOR"
   ]
  },
  {
   "cell_type": "code",
   "id": "twolayer-code",
   "metadata": {},
   "source": [
    "# With a hidden layer, the network CAN learn XOR!\n",
    "# Note: nn.Linear performs matrix multiplication — see Chapter 5 for the full explanation\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 4),    # Hidden layer: 2 inputs → 4 hidden neurons\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(4, 1),    # Output layer: 4 hidden → 1 output\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2.0)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(2000):\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Two-layer network on XOR: loss goes to zero!\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Two-layer network predictions on XOR:\")\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        pred = model(X[i]).item()\n",
    "        print(f\"  ({X[i,0].int().item()}, {X[i,1].int().item()}) → {pred:.3f}  (target: {y[i].item():.0f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "boundary-header",
   "metadata": {},
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "id": "boundary-code",
   "metadata": {},
   "source": [
    "# Visualize what the two-layer network learned\n",
    "# The decision boundary shows where the network switches from 0 to 1\n",
    "\n",
    "xx, yy = torch.meshgrid(torch.linspace(-0.5, 1.5, 100),\n",
    "                         torch.linspace(-0.5, 1.5, 100), indexing='ij')\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    zz = model(grid).reshape(100, 100)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.contourf(xx.numpy(), yy.numpy(), zz.numpy(), levels=20, cmap='RdYlBu_r', alpha=0.8)\n",
    "plt.colorbar(label='Network output')\n",
    "\n",
    "colors = ['blue', 'red', 'red', 'blue']\n",
    "for i in range(4):\n",
    "    plt.plot(X[i,0], X[i,1], 'o', color=colors[i], markersize=12,\n",
    "            markeredgecolor='black', markeredgewidth=2)\n",
    "    plt.annotate(f'XOR={y[i].item():.0f}', (X[i,0]+0.05, X[i,1]+0.08), fontsize=10)\n",
    "\n",
    "plt.xlabel(\"x₁\")\n",
    "plt.ylabel(\"x₂\")\n",
    "plt.title(\"Decision boundary: the network learned to separate XOR!\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook accompanies [Chapter 3: Neural Networks](https://robennals.github.io/ai-explained/03-neurons). The interactive widgets in the web version let you explore these concepts visually.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
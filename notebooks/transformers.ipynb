{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Transformers — Try it in PyTorch\n",
    "\n",
    "This is an **optional** hands-on companion to [Chapter 8](https://learnai.robennals.org/transformers). You'll build a complete transformer from scratch, train it on simple sequences, and then train one on real text to generate children's stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "block-header",
   "metadata": {},
   "source": [
    "## One Transformer Block\n",
    "\n",
    "Each block does two things:\n",
    "1. **Attention**: every word looks at every other word and gathers relevant context\n",
    "2. **Feed-forward network**: a small neural network that processes what attention gathered\n",
    "\n",
    "Plus a **shortcut connection** (residual) that adds the input directly to the output, so the block only needs to learn *refinements*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Step 1: Attention + residual\n",
    "        attn_out, attn_weights = self.attn(x, x, x, attn_mask=mask)\n",
    "        x = self.ln1(x + attn_out)    # shortcut connection!\n",
    "        # Step 2: Feed-forward + residual\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.ln2(x + ff_out)      # another shortcut connection\n",
    "        return x, attn_weights\n",
    "\n",
    "# Test with random input\n",
    "torch.manual_seed(42)\n",
    "block = TransformerBlock(embed_dim=16, n_heads=2, ff_dim=32)\n",
    "x = torch.randn(1, 5, 16)  # batch=1, seq_len=5, embed_dim=16\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, weights = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}  (same! the block refines, it doesn't change the shape)\")\n",
    "print(f\"Attention weights: {weights.shape}  ({weights.shape[1]}×{weights.shape[2]} attention matrix)\")\n",
    "print(f\"\\nThe shortcut connection means the output = input + refinements.\")\n",
    "print(f\"This is why you can stack many blocks without losing information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full-header",
   "metadata": {},
   "source": [
    "## A Complete Transformer\n",
    "\n",
    "Stack the pieces: embedding → position encoding → N transformer blocks → linear output. This is the full architecture behind GPT, Claude, and every modern language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_heads, ff_dim, n_layers, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, n_heads, ff_dim)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        # Embed tokens and add position\n",
    "        tok_emb = self.embedding(x)\n",
    "        pos_emb = self.pos_embedding(torch.arange(seq_len, device=x.device))\n",
    "        h = tok_emb + pos_emb\n",
    "\n",
    "        # Causal mask: each position can only attend to itself and earlier positions\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        all_weights = []\n",
    "        for block in self.blocks:\n",
    "            h, w = block(h, mask=mask)\n",
    "            all_weights.append(w)\n",
    "\n",
    "        h = self.ln_final(h)\n",
    "        logits = self.output(h)  # (batch, seq_len, vocab_size)\n",
    "        return logits, all_weights\n",
    "\n",
    "# Build a tiny transformer\n",
    "model = TinyTransformer(\n",
    "    vocab_size=16,\n",
    "    embed_dim=32,\n",
    "    n_heads=2,\n",
    "    ff_dim=64,\n",
    "    n_layers=2,\n",
    ")\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {n_params:,}\")\n",
    "print(f\"Architecture: embed(32) → 2 blocks × (2-head attention + FFN(64)) → output(16)\")\n",
    "print(f\"\\nThis is the same architecture as GPT — just much, much smaller!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern-header",
   "metadata": {},
   "source": [
    "## Training on a Simple Pattern\n",
    "\n",
    "Let's train our tiny transformer to learn a simple pattern: sequences where each token copies the one 2 positions back. This is easy enough to verify but requires the model to actually attend to the right position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data: each token copies the one 2 positions back\n",
    "# e.g., [3, 7, 3, 7, 3, 7, ...]\n",
    "def make_copy_data(n_samples=500, seq_len=8, vocab_size=16):\n",
    "    data = []\n",
    "    for _ in range(n_samples):\n",
    "        seq = torch.randint(0, vocab_size, (2,)).tolist()\n",
    "        for _ in range(seq_len - 2):\n",
    "            seq.append(seq[-2])  # copy from 2 positions back\n",
    "        data.append(seq)\n",
    "    return torch.tensor(data)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "data = make_copy_data()\n",
    "\n",
    "print(\"Example sequences (each token copies 2 positions back):\")\n",
    "for i in range(5):\n",
    "    print(f\"  {data[i].tolist()}\")\n",
    "\n",
    "# Train\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Input is all tokens except the last; target is all tokens except the first\n",
    "    x = data[:, :-1]     # input\n",
    "    y = data[:, 1:]      # target (shifted by 1)\n",
    "\n",
    "    logits, _ = model(x)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, 16), y.reshape(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss (copy-2-back task)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: does the model learn the pattern?\n",
    "model.eval()\n",
    "test = make_copy_data(n_samples=10, seq_len=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, attn_weights = model(test[:, :-1])\n",
    "    preds = logits.argmax(dim=-1)\n",
    "\n",
    "# Check accuracy on positions 2+ (where the copy pattern applies)\n",
    "targets = test[:, 1:]\n",
    "correct = (preds[:, 2:] == targets[:, 2:]).float().mean()\n",
    "print(f\"Accuracy on copy-2-back positions: {correct.item():.1%}\")\n",
    "\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Input:  {test[i, :-1].tolist()}\")\n",
    "    print(f\"  Target: {targets[i].tolist()}\")\n",
    "    print(f\"  Pred:   {preds[i].tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern-attn-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention: the model should attend to 2 positions back\n",
    "with torch.no_grad():\n",
    "    _, attn = model(test[:1, :-1])\n",
    "\n",
    "fig, axes = plt.subplots(1, len(attn), figsize=(5 * len(attn), 4))\n",
    "if len(attn) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for layer_idx, layer_attn in enumerate(attn):\n",
    "    # Average across heads\n",
    "    avg_attn = layer_attn[0].mean(dim=0).numpy()\n",
    "    ax = axes[layer_idx]\n",
    "    ax.imshow(avg_attn, cmap=\"Blues\", vmin=0)\n",
    "    ax.set_xlabel(\"Attending to position\")\n",
    "    ax.set_ylabel(\"From position\")\n",
    "    ax.set_title(f\"Layer {layer_idx + 1} attention (avg over heads)\")\n",
    "\n",
    "plt.suptitle(\"The model learns to attend to 2 positions back\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Look for a diagonal pattern shifted by 2 — that's the model looking\")\n",
    "print(\"back 2 positions to find the token it should copy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stories-header",
   "metadata": {},
   "source": [
    "## From Patterns to Stories\n",
    "\n",
    "The same architecture, trained on real text instead of symbol patterns, learns language. Let's train on a small corpus of children's story beginnings and watch it generate text.\n",
    "\n",
    "This is a *tiny* model on a *tiny* dataset — the output won't be Shakespeare. But you'll see the same predict-the-next-token loop that powers ChatGPT and Claude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stories-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small corpus of story beginnings (character-level for simplicity)\n",
    "stories = [\n",
    "    \"once upon a time there was a little cat who loved to play\",\n",
    "    \"once upon a time there was a little dog who loved to run\",\n",
    "    \"once upon a time there was a little girl named lily\",\n",
    "    \"once upon a time there was a little boy named tom\",\n",
    "    \"the cat sat on the mat and looked at the bird\",\n",
    "    \"the dog ran in the park and played with the ball\",\n",
    "    \"she was very happy because she found a new friend\",\n",
    "    \"he was very sad because he lost his toy\",\n",
    "    \"the little girl smiled and said hello to everyone\",\n",
    "    \"the little boy laughed and ran around the garden\",\n",
    "    \"one day the cat found a box and climbed inside\",\n",
    "    \"one day the dog found a bone and was very happy\",\n",
    "    \"they played together all day and had so much fun\",\n",
    "    \"the sun was shining and the birds were singing\",\n",
    "    \"she loved her cat and her cat loved her\",\n",
    "    \"he loved his dog and his dog loved him\",\n",
    "]\n",
    "\n",
    "# Build a word-level vocabulary\n",
    "all_words = sorted(set(w for s in stories for w in s.split()))\n",
    "word2id = {w: i + 1 for i, w in enumerate(all_words)}  # 0 = padding\n",
    "word2id[\"<pad>\"] = 0\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "print(f\"Vocabulary: {vocab_size} words\")\n",
    "\n",
    "# Tokenize stories\n",
    "max_len = max(len(s.split()) for s in stories)\n",
    "tokenized = torch.zeros(len(stories), max_len, dtype=torch.long)\n",
    "for i, story in enumerate(stories):\n",
    "    ids = [word2id[w] for w in story.split()]\n",
    "    tokenized[i, :len(ids)] = torch.tensor(ids)\n",
    "\n",
    "print(f\"Max sequence length: {max_len}\")\n",
    "print(f\"Example: {stories[0][:40]}...\")\n",
    "print(f\"Tokens:  {tokenized[0, :8].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stories-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a small transformer on stories\n",
    "torch.manual_seed(42)\n",
    "story_model = TinyTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=32,\n",
    "    n_heads=2,\n",
    "    ff_dim=64,\n",
    "    n_layers=2,\n",
    "    max_len=max_len,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(story_model.parameters(), lr=0.003)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(500):\n",
    "    x = tokenized[:, :-1]\n",
    "    y = tokenized[:, 1:]\n",
    "\n",
    "    logits, _ = story_model(x)\n",
    "    # Ignore padding tokens in the loss\n",
    "    loss = F.cross_entropy(logits.reshape(-1, vocab_size), y.reshape(-1), ignore_index=0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss (story generation)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-header",
   "metadata": {},
   "source": [
    "## Auto-regressive Generation\n",
    "\n",
    "Now the fun part: predict → append → repeat. This is exactly how every large language model generates text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, max_new_tokens=15, temperature=0.8):\n",
    "    \"\"\"Generate text auto-regressively: predict, append, repeat.\"\"\"\n",
    "    model.eval()\n",
    "    tokens = [word2id[w] for w in prompt.split() if w in word2id]\n",
    "    ids = torch.tensor([tokens])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = model(ids)\n",
    "            # Take prediction for the last position\n",
    "            next_logits = logits[0, -1] / max(temperature, 0.01)\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1)\n",
    "            if next_id.item() == 0:  # stop on padding\n",
    "                break\n",
    "            ids = torch.cat([ids, next_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "    return \" \".join(id2word[i.item()] for i in ids[0] if i.item() != 0)\n",
    "\n",
    "# Generate from different prompts\n",
    "prompts = [\"once upon a\", \"the little\", \"she was\", \"one day the\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    for i in range(3):\n",
    "        text = generate(story_model, prompt, temperature=0.7)\n",
    "        print(f\"  → {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temperature-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature comparison\n",
    "prompt = \"once upon a time\"\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "for temp in [0.1, 0.5, 1.0, 2.0]:\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    for _ in range(3):\n",
    "        text = generate(story_model, prompt, temperature=temp)\n",
    "        print(f\"  {text}\")\n",
    "    print()\n",
    "\n",
    "print(\"Low temperature → repetitive and safe.\")\n",
    "print(\"High temperature → creative but chaotic.\")\n",
    "print(\"This is the same trade-off in ChatGPT and Claude!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale-header",
   "metadata": {},
   "source": [
    "## The Power of Scale\n",
    "\n",
    "Our tiny model has a few thousand parameters and was trained on 16 sentences. GPT-4 has over a **trillion** parameters trained on trillions of words. The architecture is the same — embed, attend, transform, predict — the difference is just scale.\n",
    "\n",
    "Let's see how even a small increase in model size helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"Tiny\",   16, 1, 32,  1),\n",
    "    (\"Small\",  32, 2, 64,  2),\n",
    "    (\"Medium\", 64, 4, 128, 3),\n",
    "]\n",
    "\n",
    "print(f\"{'Name':<8} {'Embed':>6} {'Heads':>6} {'FF':>6} {'Layers':>7} {'Params':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for name, emb, heads, ff, layers in configs:\n",
    "    m = TinyTransformer(vocab_size, emb, heads, ff, layers)\n",
    "    n = sum(p.numel() for p in m.parameters())\n",
    "    print(f\"{name:<8} {emb:>6} {heads:>6} {ff:>6} {layers:>7} {n:>10,}\")\n",
    "\n",
    "print(f\"\\nGPT-3: 175,000,000,000 parameters\")\n",
    "print(f\"GPT-4: ~1,800,000,000,000 parameters\")\n",
    "print(f\"\\nSame architecture. Same training task (predict the next token).\")\n",
    "print(f\"Scale is what turns a toy into an AI that can write, reason, and code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": "---\n\n*This notebook accompanies [Chapter 8: Transformers](https://learnai.robennals.org/transformers). The interactive widgets in the web version let you step through a toy transformer, explore attention patterns on real stories, and generate text with temperature control.*\n\n*New to PyTorch? See the [PyTorch from Scratch](https://learnai.robennals.org/appendix-pytorch) appendix for a beginner-friendly introduction.*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Predicting the Next Word — Try it in PyTorch\n",
    "\n",
    "This is an **optional** hands-on companion to [Chapter 6](https://robennals.github.io/ai-explained/06-next-word-prediction). You'll build bigram models, see why lookup tables hit a wall, and train a neural network that generalizes through embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigram-header",
   "metadata": {},
   "source": [
    "## Bigram Model: Counting What Comes Next\n",
    "\n",
    "The simplest next-word predictor: for every word, count what word usually follows it. This is just a big lookup table of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigram-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny corpus of children's stories\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the rug\",\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"she was very happy\",\n",
    "    \"he was very sad\",\n",
    "    \"she was very kind\",\n",
    "    \"once upon a time there was a little girl\",\n",
    "    \"once upon a time there was a little boy\",\n",
    "    \"once upon a time there was a little cat\",\n",
    "    \"the little girl was happy\",\n",
    "    \"the little boy was sad\",\n",
    "]\n",
    "\n",
    "# Count bigrams: how often does word B follow word A?\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "bigram_counts = defaultdict(Counter)\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for a, b in zip(words, words[1:]):\n",
    "        bigram_counts[a][b] += 1\n",
    "\n",
    "# Show predictions for some words\n",
    "for word in [\"the\", \"was\", \"once\", \"little\"]:\n",
    "    total = sum(bigram_counts[word].values())\n",
    "    print(f\"After '{word}':\")\n",
    "    for next_word, count in bigram_counts[word].most_common(5):\n",
    "        print(f\"  {next_word:>8}: {count/total:.0%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-bigram-header",
   "metadata": {},
   "source": [
    "## Generating from a Bigram Model\n",
    "\n",
    "We can chain predictions together to generate text. Each pair of words sounds fine, but the whole thing is a random walk through common word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-bigram-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_bigram(start_word, length=15):\n",
    "    words = [start_word]\n",
    "    current = start_word\n",
    "    for _ in range(length):\n",
    "        if current not in bigram_counts:\n",
    "            break\n",
    "        options = bigram_counts[current]\n",
    "        total = sum(options.values())\n",
    "        # Sample proportionally to frequency\n",
    "        r = random.random() * total\n",
    "        cumulative = 0\n",
    "        for word, count in options.items():\n",
    "            cumulative += count\n",
    "            if r <= cumulative:\n",
    "                words.append(word)\n",
    "                current = word\n",
    "                break\n",
    "    return \" \".join(words)\n",
    "\n",
    "print(\"Generated sentences:\")\n",
    "for start in [\"the\", \"once\", \"she\"]:\n",
    "    for i in range(3):\n",
    "        print(f\"  {generate_bigram(start)}\")\n",
    "    print()\n",
    "\n",
    "print(\"Each word pair is reasonable, but the whole sentence is nonsense!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ngram-wall-header",
   "metadata": {},
   "source": [
    "## The N-gram Wall\n",
    "\n",
    "More context should help, right? Trigrams (3 words) are better than bigrams, and 4-grams are better still. But the number of possible sequences explodes so fast that most sequences are never seen — even in huge datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ngram-wall-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50_000  # Realistic English vocabulary\n",
    "\n",
    "print(\"Number of possible n-gram entries:\")\n",
    "print(f\"{'N':>4}  {'Combinations':>20}  {'Comparison'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "comparisons = [\n",
    "    (2, \"manageable\"),\n",
    "    (3, \"large but feasible\"),\n",
    "    (4, \"bigger than Wikipedia\"),\n",
    "    (5, \"more than all books ever written\"),\n",
    "    (10, \"more than atoms in the universe\"),\n",
    "]\n",
    "\n",
    "for n, comparison in comparisons:\n",
    "    combos = vocab_size ** n\n",
    "    print(f\"{n:>4}  {combos:>20.2e}  {comparison}\")\n",
    "\n",
    "print(f\"\\nWith a vocab of {vocab_size:,} words, 10-grams need {vocab_size**10:.0e} entries.\")\n",
    "print(\"There aren't enough books in the world to fill that table!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn-header",
   "metadata": {},
   "source": [
    "## Neural Network to the Rescue\n",
    "\n",
    "Instead of a lookup table, use a neural network. Feed it the *embeddings* of the previous words, pass through a hidden layer, and output a probability for each word in the vocabulary.\n",
    "\n",
    "The key advantage: the network **generalizes**. \"Cat\" and \"dog\" have similar embeddings, so a network that learns \"the cat → sat\" will automatically make similar predictions for \"the dog\" — even if it never saw that exact input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tiny vocabulary and training data\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\"],\n",
    "    [\"the\", \"dog\", \"sat\"],\n",
    "    [\"the\", \"cat\", \"ran\"],\n",
    "    [\"the\", \"dog\", \"ran\"],\n",
    "    [\"the\", \"bird\", \"flew\"],\n",
    "    [\"a\", \"cat\", \"sat\"],\n",
    "    [\"a\", \"dog\", \"ran\"],\n",
    "    [\"the\", \"girl\", \"smiled\"],\n",
    "    [\"the\", \"boy\", \"smiled\"],\n",
    "    [\"the\", \"girl\", \"ran\"],\n",
    "    [\"the\", \"boy\", \"ran\"],\n",
    "    [\"she\", \"was\", \"happy\"],\n",
    "    [\"he\", \"was\", \"happy\"],\n",
    "    [\"she\", \"was\", \"sad\"],\n",
    "    [\"he\", \"was\", \"sad\"],\n",
    "]\n",
    "\n",
    "# Create vocabulary\n",
    "all_words = sorted(set(w for s in sentences for w in s))\n",
    "word2id = {w: i for i, w in enumerate(all_words)}\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary ({vocab_size} words): {all_words}\")\n",
    "\n",
    "# Training data: context of 2 words → predict 3rd\n",
    "context_len = 2\n",
    "X = torch.tensor([[word2id[w] for w in s[:context_len]] for s in sentences])\n",
    "Y = torch.tensor([word2id[s[context_len]] for s in sentences])\n",
    "print(f\"Training examples: {len(X)}\")\n",
    "print(f\"Example: {[id2word[i.item()] for i in X[0]]} → {id2word[Y[0].item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: embedding → flatten → dense → softmax\n",
    "class NextWordModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, context_len, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(context_len * embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)            # (batch, context_len, embed_dim)\n",
    "        e = e.view(e.size(0), -1)        # flatten to (batch, context_len * embed_dim)\n",
    "        h = F.relu(self.fc1(e))          # hidden layer with ReLU\n",
    "        return self.fc2(h)               # scores for each word in vocab\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = NextWordModel(vocab_size, embed_dim=8, context_len=2, hidden_dim=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params}\")\n",
    "print(f\"Architecture: Embedding(dim=8) → Dense(16, ReLU) → Dense({vocab_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "losses = []\n",
    "for epoch in range(300):\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, Y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "\n",
    "test_contexts = [\n",
    "    [\"the\", \"cat\"],\n",
    "    [\"the\", \"dog\"],\n",
    "    [\"the\", \"bird\"],\n",
    "    [\"the\", \"girl\"],\n",
    "    [\"the\", \"boy\"],\n",
    "    [\"she\", \"was\"],\n",
    "    [\"he\", \"was\"],\n",
    "]\n",
    "\n",
    "print(\"Predictions:\")\n",
    "with torch.no_grad():\n",
    "    for ctx in test_contexts:\n",
    "        ids = torch.tensor([[word2id[w] for w in ctx]])\n",
    "        logits = model(ids)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top3 = probs.topk(3, dim=-1)\n",
    "        preds = [(id2word[i.item()], p.item()) for i, p in zip(top3.indices[0], top3.values[0])]\n",
    "        pred_str = \", \".join(f\"{w}({p:.0%})\" for w, p in preds)\n",
    "        print(f\"  {ctx[0]} {ctx[1]} → {pred_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generalize-header",
   "metadata": {},
   "source": [
    "## Generalization Through Embeddings\n",
    "\n",
    "The whole point of using a neural network: similar words produce similar predictions automatically. Let's visualize the learned embeddings to see *why* — similar words ended up close together in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generalize-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize the learned embeddings (project to 2D with PCA)\n",
    "with torch.no_grad():\n",
    "    embeddings = model.embedding.weight.numpy()\n",
    "\n",
    "# Simple 2D projection using SVD (like PCA)\n",
    "centered = embeddings - embeddings.mean(axis=0)\n",
    "U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "coords = centered @ Vt[:2].T\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, word in enumerate(all_words):\n",
    "    plt.plot(coords[i, 0], coords[i, 1], 'o', markersize=10)\n",
    "    plt.annotate(word, (coords[i, 0] + 0.02, coords[i, 1] + 0.02), fontsize=11)\n",
    "\n",
    "plt.title(\"Learned embeddings (projected to 2D)\\nSimilar words cluster together\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Animals (cat, dog, bird) should cluster. People (girl, boy) should cluster.\")\n",
    "print(\"The network learned these groupings automatically — nobody told it cats and dogs are similar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temperature-header",
   "metadata": {},
   "source": [
    "## Temperature Sampling\n",
    "\n",
    "When generating text, we don't always want the most likely word — that makes boring, repetitive text. **Temperature** controls randomness: low temperature = safe and predictable, high temperature = creative and wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temperature-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_temperature(model, context, temperature=1.0):\n",
    "    \"\"\"Sample a next word using temperature-scaled probabilities.\"\"\"\n",
    "    ids = torch.tensor([[word2id[w] for w in context]])\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids)\n",
    "        # Divide logits by temperature before softmax\n",
    "        scaled = logits / max(temperature, 0.01)\n",
    "        probs = F.softmax(scaled, dim=-1).squeeze()\n",
    "    # Sample from the distribution\n",
    "    idx = torch.multinomial(probs, 1).item()\n",
    "    return id2word[idx]\n",
    "\n",
    "# Show how temperature affects predictions\n",
    "context = [\"the\", \"cat\"]\n",
    "for temp in [0.1, 0.5, 1.0, 2.0]:\n",
    "    samples = [predict_with_temperature(model, context, temp) for _ in range(20)]\n",
    "    counts = Counter(samples)\n",
    "    dist = \", \".join(f\"{w}: {c}\" for w, c in counts.most_common())\n",
    "    print(f\"Temperature {temp:.1f}: {dist}\")\n",
    "\n",
    "print(\"\\nLow temp → always picks the top word. High temp → more variety.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook accompanies [Chapter 6: Predicting the Next Word](https://robennals.github.io/ai-explained/06-next-word-prediction). The interactive widgets in the web version let you explore these concepts visually — including a real neural network trained on 50,000 children's stories.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
